---
marp: true
theme: default
paginate: true
math: mathjax
style: @import "custom.css";
---

<!-- _class: lead -->
<!-- _paginate: false -->

# Active Learning

**CS 203: Software Tools and Techniques for AI**
Prof. Nipun Batra, IIT Gandhinagar

---

# The Labeling Problem

**Scenario**: You need to train a classifier

**Traditional approach:**
1. Collect 10,000 images
2. Label all 10,000 images
3. Train model
4. Hope it works

**The cost:**
- 10,000 labels × 30 seconds = 83 hours
- At $20/hour = $1,660
- Many labels are redundant

**Question**: Can we be smarter about which data to label?

---

# What is Active Learning?

**Active Learning**: Intelligently select which examples to label to maximize model performance with minimal labeling effort

**Key Insight**: Not all data points are equally valuable for learning!

**Example:**
- 100 random samples might give 85% accuracy
- 100 carefully chosen samples might give 92% accuracy

**Goal**: Achieve same performance with 5-10× fewer labels

---

# Passive vs Active Learning

![Active Learning Comparison](../figures/active_learning_comparison.png)

*Generated by: [diagram-generators/active_learning_comparison.py](../diagram-generators/active_learning_comparison.py)*

---

# When to Use Active Learning

**Use Active Learning when:**
- Labeling is expensive (human time, expert knowledge)
- You have large unlabeled dataset
- You need good performance with limited labels
- Labels are imbalanced or rare

**Real-world applications:**
- Medical imaging (radiologist time is expensive)
- Legal document review (lawyer expertise)
- Rare event detection (fraud, defects)
- Custom domain classification

**Don't use when:**
- Labeling is cheap/free
- Small dataset (just label everything)
- Labels already available

---

# Active Learning Cycle

**The Loop:**

1. **Start**: Train initial model on small labeled set
2. **Query**: Select most informative unlabeled examples
3. **Oracle**: Human labels selected examples
4. **Update**: Retrain model with new labels
5. **Repeat**: Until performance target or budget reached

---
# Active Learning Cycle

**Key components:**
- **Learner**: ML model being trained
- **Query Strategy**: How to select examples
- **Oracle**: Human labeler (or simulation)
- **Pool**: Unlabeled data to select from

---
# Query Strategies: Comparison

<style>
table {
  font-size: 0.80em;
}
</style>

| Strategy | Approach | Pros | Cons | Best For |
|---------|----------|------|------|---------|
| **Uncertainty Sampling** | Pick most uncertain examples | Simple, fast, effective | May cluster in decision boundary | Most tasks (default choice) |
| **Query-by-Committee** | Pick where models disagree | Robust, diverse | Requires training multiple models | When you can afford ensembles |
| **Expected Model Change** | Pick examples that change model most | Adaptive | Computationally expensive | When labels are very expensive |
| **Diversity Sampling** | Pick diverse examples | Covers feature space | Ignores model uncertainty | Imbalanced or clustered data |
| **Hybrid** | Combine uncertainty + diversity | Best of both worlds | More complex | Production systems |

**Recommendation**: Start with **Uncertainty Sampling**, add diversity if needed

---

# Uncertainty Sampling

**Idea**: Label examples where the model is most confused

**For binary classification:**
- Model predicts P(positive) = 0.51
- Model is uncertain! Label this example

**For multi-class:**
- Model predicts [0.34, 0.33, 0.33]
- Very uncertain! Label this

**Intuition**: Easy examples don't teach us much. Hard examples are informative.

---

# Uncertainty Measures: Mathematical Foundation

## Problem Setup

Given:
- Model $f$ with parameters $\theta$
- Unlabeled example $x$
- Class predictions $P_\theta(y|x)$ for classes $y \in \{1, ..., C\}$

**Goal**: Define uncertainty $U(x)$ to select most informative examples

---

# Uncertainty Measure 1: Least Confident

## Formula

$$U_{LC}(x) = 1 - P_\theta(\hat{y}|x)$$

where $\hat{y} = \arg\max_y P_\theta(y|x)$ is the predicted class

## Interpretation

- Measures how uncertain the model is about its top prediction
- Range: [0, 1]
- High value = low confidence = select for labeling
---

**Example 1:**
Probabilities: $P(y|x) = [0.6, 0.3, 0.1]$

$$U_{LC}(x) = 1 - 0.6 = 0.4$$


**Example 2:** 
Probabilities: $P(y|x) = [0.95, 0.03, 0.02]$

$$U_{LC}(x) = 1 - 0.95 = 0.05$$


**Example 3:**  
Probabilities: $P(y|x) = [0.4, 0.35, 0.25]$

$$U_{LC}(x) = 1 - 0.4 = 0.6$$

**Which is most certain and most uncertain?**

---

# Uncertainty Measure 2: Margin Sampling

## Formula

$$U_M(x) = P_\theta(\hat{y}_1|x) - P_\theta(\hat{y}_2|x)$$

where:
- $\hat{y}_1$ = most probable class
- $\hat{y}_2$ = second most probable class

Then uncertainty is:
$$U_{M}^{inv}(x) = 1 - U_M(x)$$

---
## Interpretation

- Small margin = model is confused between top 2 classes
- Margin close to 0 → most uncertain
- Better than least confident for multiclass

---

# Margin Sampling: Example


**Example A**: $P(y|x) = [0.51, 0.49, 0.00]$
$$U_M(A) = 0.51 - 0.49 = 0.02 \quad \text{(very small margin)}$$

**Example B**: $P(y|x) = [0.99, 0.01, 0.00]$
$$U_M(B) = 0.99 - 0.01 = 0.98 \quad \text{(large margin)}$$

**Selection**: Example A is more uncertain → select for labeling

**Comparison with Least Confident**:
- $U_{LC}(A) = 1 - 0.51 = 0.49$
- $U_{LC}(B) = 1 - 0.99 = 0.01$

Both correctly identify A as more uncertain!

---

# Uncertainty Measure 3: Entropy

## Formula

$$H(P_\theta(y|x)) = -\sum_{y=1}^{C} P_\theta(y|x) \log P_\theta(y|x)$$

## Interpretation

- Measures disorder/uncertainty in probability distribution
- Range: $[0, \log C]$
- Maximum when all classes equally likely
- Most theoretically principled measure

---
## Properties

For $C$ classes:
- **Min entropy** (certain): $H = 0$ when $P = [1, 0, ..., 0]$
- **Max entropy** (uncertain): $H = \log C$ when $P = [1/C, ..., 1/C]$

---

# Entropy: Detailed Example

## Binary Classification ($C = 2$)

**Example 1** (very confident):
$$P(y|x) = [0.95, 0.05]$$
$$H = -0.95\log(0.95) - 0.05\log(0.05) = 0.286$$

**Example 2** (uncertain):
$$P(y|x) = [0.5, 0.5]$$
$$H = -0.5\log(0.5) - 0.5\log(0.5) = 0.693$$

**Maximum possible**: $\log(2) = 0.693$

**Selection**: Example 2 has higher entropy → more uncertain

---

# Entropy: Multi-Class Example
**3-Class Classification ($C = 3$)**

**Example A** (confident): $P(y|x) = [0.8, 0.15, 0.05]$
$$H_A = -(0.8 \log 0.8 + 0.15 \log 0.15 + 0.05 \log 0.05) = 0.849$$

**Example B** (uncertain): $P(y|x) = [0.4, 0.35, 0.25]$
$$H_B = -(0.4 \log 0.4 + 0.35 \log 0.35 + 0.25 \log 0.25) = 1.571$$

**Example C** (max uncertainty): $P(y|x) = [0.33, 0.33, 0.33]$
$$H_C = -3 \times (0.33 \log 0.33) = 1.585 $$

**Selection**: $H_C > H_B > H_A$ → Example C most uncertain


---

# Comparing Uncertainty Measures
<div style="font-size:1.5em;">

| Measure | Formula | Best For | Limitations |
|---------|---------|----------|-------------|
| **Least Confident** | $1 - \max_y P(y\|x)$ | Simple, fast | Ignores distribution shape |
| **Margin** | $P(\hat{y}_1\|x) - P(\hat{y}_2\|x)$ | Binary/multiclass | Only considers top 2 |
| **Entropy** | $-\sum_y P(y\|x) \log P(y\|x)$ | Full distribution | More computation |

</div>


---
## Example Comparison

$P(y|x) = [0.6, 0.3, 0.05, 0.05]$

- Least Confident: $U = 0.4$
- Margin: $U = 0.3$
- Entropy: $H = 1.23$

All identify this as moderately uncertain

---

# Uncertainty Sampling - Code

**Basic implementation:**

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

---

# Query-by-Committee: Mathematical Foundation

## Setup

**Committee**: $\mathcal{C} = \{h_1, h_2, ..., h_M\}$ of $M$ models
- Each $h_i$ trained on same labeled data $\mathcal{L}$
- Different algorithms or random initializations

**For unlabeled example** $x$:
- Get prediction distribution from each model: $P_{h_i}(y|x)$

**Goal**: Measure disagreement among committee members

---

# QBC Disagreement Measure 1: Vote Entropy

## Formula

$$D_{VE}(x) = -\sum_{y=1}^{C} \frac{V(y)}{M} \log \frac{V(y)}{M}$$

where $V(y)$ = number of committee members voting for class $y$

---


## Example

Committee of 5 models, binary classification:
- 3 models predict class 0
- 2 models predict class 1

$$V(0) = 3, \quad V(1) = 2$$

$$D_{VE}(x) = -\left(\frac{3}{5}\log\frac{3}{5} + \frac{2}{5}\log\frac{2}{5}\right) = 0.673$$

**Maximum disagreement**: When votes are split equally

---

# QBC Disagreement Measure 2: Consensus Entropy


Average prediction distribution across committee:

$$P_C(y|x) = \frac{1}{M} \sum_{i=1}^{M} P_{h_i}(y|x)$$

Then calculate entropy of consensus:

$$D_{CE}(x) = H(P_C(y|x)) = -\sum_{y=1}^{C} P_C(y|x) \log P_C(y|x)$$

---

## Interpretation

- Uses full probability distributions, not just votes
- More information than vote entropy
- Higher values = more disagreement

---

# QBC: Detailed Example

## Scenario: 3 Models, Binary Classification

|  | $P(y=0\|x)$ | $P(y=1\|x)$ |
|--|-----------|-----------|
| Model 1 | 0.9 | 0.1 |
| Model 2 | 0.4 | 0.6 |
| Model 3 | 0.3 | 0.7 |

## Calculate Consensus Distribution ?
## Calculate Consensus Entropy ?

---

## Calculate Consensus Distribution

$$P_C(y=0|x) = \frac{0.9 + 0.4 + 0.3}{3} = 0.533$$

$$P_C(y=1|x) = \frac{0.1 + 0.6 + 0.7}{3} = 0.467$$

## Calculate Consensus Entropy

$$D_{CE}(x) = -(0.533 \log 0.533 + 0.467 \log 0.467) = 0.991$$

**High entropy** → models disagree → select for labeling

---

# QBC Disagreement Measure 3: KL Divergence


Measure divergence of each model from consensus:

$$D_{KL}(x) = \frac{1}{M} \sum_{i=1}^{M} KL(P_{h_i}(y|x) \| P_C(y|x))$$

where KL divergence is:

$$KL(P \| Q) = \sum_{y} P(y) \log \frac{P(y)}{Q(y)}$$

---

## Interpretation

- Measures how much individual predictions differ from average
- Higher values = more disagreement
- Theoretically motivated (information theory)

---

# KL Divergence: Example

Using previous example, consensus is $P_C = [0.533, 0.467]$

## Model 1: $P_{h_1} = [0.9, 0.1]$

$$KL_1 = 0.9 \log\frac{0.9}{0.533} + 0.1 \log\frac{0.1}{0.467} = 0.397$$

## Model 2: $P_{h_2} = [0.4, 0.6]$

$$KL_2 = 0.4 \log\frac{0.4}{0.533} + 0.6 \log\frac{0.6}{0.467} = 0.056$$

---

## Model 3: $P_{h_3} = [0.3, 0.7]$

$$KL_3 = 0.3 \log\frac{0.3}{0.533} + 0.7 \log\frac{0.7}{0.467} = 0.130$$

## Average KL:
$$D_{KL}(x) = \frac{0.397 + 0.056 + 0.130}{3} = 0.194$$

---

# Query-by-Committee - Code

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Expected Model Change

**Idea**: Select examples that will change the model parameters most if labeled

**Approach:**
- For each unlabeled example, simulate adding it with each possible label
- Measure how much model parameters change
- Select examples causing largest change

---
## Gradient-based:
```python
# For each example x:
gradient = model.compute_gradient(x)
impact = ||gradient||  # Magnitude of gradient
```

**Pros**: Directly optimizes for model learning
**Cons**: Computationally expensive (need to retrain or compute gradients)

---

# Diversity Sampling

**Problem**: Uncertainty sampling can select similar examples

**Solution**: Also consider diversity

**Approaches:**

1. **K-means clustering**: Select one example from each cluster
2. **Core-set selection**: Select examples that best represent all data
3. **Hybrid**: Combine uncertainty + diversity

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Cold Start Problem

**Challenge**: How to start with no labeled data?


1. **Random Sampling**: Label small random set to bootstrap
   ```python
   # Start with 20-50 random examples
   initial_indices = np.random.choice(len(X_pool), size=20, replace=False)
   X_labeled = X_pool[initial_indices]
   ```

2. **Cluster-based**: Sample from each cluster
   ```python
   kmeans = KMeans(n_clusters=10)
   kmeans.fit(X_pool)
   # Select one from each cluster
   ```

3. **Representative Sampling**: Use diversity methods

**Rule of thumb**: Start with 5-10 examples per class

---


# Complete Active Learning Example

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

---

# Simulating Oracles

**For experiments, we need to simulate human labeling**

**Using existing labels:**
```python
def oracle(X_query, y_true, query_indices):
    # Return true labels for queried examples
    return y_true[query_indices]
```

**With noise:**
```python
def noisy_oracle(y_true, query_indices, error_rate=0.1):
    labels = y_true[query_indices].copy()
    # Flip some labels randomly
    n_errors = int(len(labels) * error_rate)
    error_idx = np.random.choice(len(labels), size=n_errors, replace=False)
    labels[error_idx] = 1 - labels[error_idx]  # Flip binary labels
    return labels
```
---

**With cost:**
```python
def oracle_with_cost(X_query, y_true, query_indices, cost_per_label=1.0):
    labels = y_true[query_indices]
    cost = len(labels) * cost_per_label
    return labels, cost
```

---

# Measuring Active Learning Performance

**Learning Curve**: Accuracy vs. number of labeled samples
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


**Metrics:**
- Accuracy at fixed budget
- Labels needed for target accuracy
- Area under learning curve

---

# Stopping Criteria

**When to stop active learning?**

1. **Budget exhausted**: Used all labeling budget
2. **Performance plateau**: Accuracy not improving
3. **Uncertainty threshold**: All examples have low uncertainty
4. **Time limit**: Deadline reached

**Automatic stopping:**
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Active Learning for Deep Learning

**Challenges:**
- Deep models need more data
- Training is expensive
- Uncertainty estimation harder

---
**Strategies:**

1. **MC Dropout**: Use dropout at inference for uncertainty
   ```python
   # Enable dropout at test time
   model.train()
   predictions = [model(x) for _ in range(30)]
   uncertainty = np.std(predictions, axis=0)
   ```

2. **Ensemble**: Train multiple models
3. **Batch acquisition**: Query batches instead of one at a time
4. **Transfer learning**: Start with pretrained model

---

# Batch Mode Active Learning

**Problem**: Querying one example at a time is inefficient for deep learning

**Solution**: Query batches of examples

**Challenge**: Selected examples might be similar

**Approaches:**

1. **Top-k uncertain**: Simple, but may select similar examples
2. **Diverse batch**: Ensure batch covers feature space
3. **BatchBALD**: Maximize information about model parameters
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Active Learning with Label Studio

**Label Studio**: Open-source annotation tool with active learning

**Features:**
- Visual interface for labeling
- Built-in active learning
- Custom ML backends
- Export to various formats

---
**Workflow:**
1. Upload unlabeled data to Label Studio
2. Connect ML model
3. Model suggests next samples to label
4. Human labels in UI
5. Model retrains automatically
6. Repeat

**Perfect for production active learning systems**

---

# Cost-Effectiveness Analysis

**Compare labeling costs:**

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


**Typical results**: 50-80% reduction in labeling costs

---

# Domain Adaptation with Active Learning

**Scenario**: Model trained on domain A, deploying to domain B

**Problem**: Distribution shift causes poor performance

**Solution**: Use active learning to select examples from domain B

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

---

# Active Learning for Imbalanced Data

**Problem**: Rare classes get few queries with standard uncertainty sampling

**Solution**: Class-balanced active learning
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>



---

# Common Pitfalls

**1. Not evaluating on separate test set**
- Always use held-out test data
- Don't evaluate on the pool

**2. Biased initial sample**
- Start with diverse/representative sample
- Not just easiest examples

**3. Ignoring computational cost**
- Querying and retraining takes time
- Budget for compute, not just labels

---
**4. Over-querying similar examples**
- Use diversity-aware strategies
- Balance uncertainty and diversity

**5. Not considering label noise**
- Real annotators make mistakes
- Build in quality checks

---

# Active Learning Best Practices

**1. Start small**: Begin with 5-10 examples per class

**2. Batch wisely**: Query 10-100 examples at once (depends on budget)

**3. Validate strategy**: Compare to random baseline

**4. Monitor convergence**: Track learning curves

**5. Consider human factors**:
   - Annotation fatigue
   - Label quality over time
   - Break large batches into sessions

**6. Save everything**: Log all queries and labels for analysis

---

# Tools and Libraries

**Active Learning:**
- **modAL**: Python active learning framework
- **alipy**: Comprehensive active learning toolkit
- **libact**: C++ based, Python bindings

---
**Annotation:**
- **Label Studio**: Web-based with active learning
- **Prodigy**: Commercial, scriptable
- **CVAT**: Computer vision annotation
- **Labelbox**: Enterprise solution

**Experiment tracking:**
- **Weights & Biases**: Track experiments
- **MLflow**: Model versioning

---

# Real-World Case Studies

**1. Medical Imaging (Chest X-rays)**
- Random: 5,000 labels for 85% accuracy
- Active: 1,500 labels for 85% accuracy
- Savings: 70% reduction in radiologist time

**2. Legal Document Review**
- Random: 10,000 documents reviewed
- Active: 3,000 documents for same recall
- Savings: $140,000 in lawyer fees

---
**3. Manufacturing Defect Detection**
- Random: 1% defect rate, need 10,000 labels
- Active: Focused on defects, 3,000 labels
- Result: 67% cost reduction

**4. Autonomous Driving (Object Detection)**
- Random: 20,000 frames annotated for stable performance
- Active: 6,000 high-uncertainty frames
- Result: 70% reduction in annotation effort

---

# Active Learning vs Other Approaches

**Active Learning vs Semi-Supervised Learning:**
- Active: Choose what to label
- Semi-supervised: Use unlabeled data directly

**Active Learning vs Transfer Learning:**
- Active: Label task-specific data intelligently
- Transfer: Use pretrained models

**Active Learning vs Few-Shot Learning:**
- Active: Iteratively grow labeled set
- Few-shot: Learn from very few examples (5-10)

**Can combine!** Transfer learning + active learning is powerful

---

# Research Directions

**Current trends:**

1. **Deep active learning**: Better uncertainty for neural nets
2. **Active learning + RL**: Learn query strategy with RL
3. **Human-in-the-loop**: Better human-AI interaction
4. **Active learning at scale**: Billion-sample pools
5. **Weak supervision**: Combine with programmatic labeling

---

# Open problems:
- Theoretical guarantees
- Better uncertainty estimation
- Handling label noise
- Multi-modal active learning

---

# Implementing Your First Active Learning System

**Step-by-step:**

1. **Load data**: Split into initial labeled set and pool
2. **Train initial model**: Use random sample
3. **Active learning loop**:
   - Predict on pool
   - Calculate uncertainty
   - Select top-k
   - Get labels (oracle or human)
   - Add to training set
   - Retrain model

---
4. **Evaluate**: Compare to random baseline
5. **Visualize**: Plot learning curves

**Start simple**: Use uncertainty sampling with logistic regression

---

# Practical Tips for Your Project

**1. Baseline is crucial**: Always compare to random sampling

**2. Start with toy dataset**: Test strategy on iris/digits

**3. Use existing labels**: Simulate oracle with held-out labels

**4. Track everything**:
   - Which samples were queried
   - Model performance at each iteration
   - Time spent

**5. Visualize uncertainty**: Plot samples by uncertainty to understand strategy

**6. Try multiple strategies**: Uncertainty, QBC, diversity

---

# What We've Learned

**Core Concepts:**
- Active learning reduces labeling costs by 50-80%
- Query strategies: Uncertainty, QBC, diversity
- Oracle simulation for experiments
- Learning curves measure performance

**Practical Skills:**
- Implementing uncertainty sampling
- Building active learning loop
- Evaluating with learning curves
- Using libraries like modAL

**Real-World:**
- Cost-effectiveness analysis
- Domain adaptation
- Imbalanced data
- Production systems with Label Studio

---

# Resources

**Papers:**
- "Active Learning Literature Survey" by Settles (2009)
- "Deep Active Learning" surveys
- "A Survey of Deep Active Learning" (2020)

**Libraries:**
- modAL: https://modal-python.readthedocs.io/
- Label Studio: https://labelstud.io/
- alipy: https://github.com/NUAA-AL/alipy

**Datasets:**
- MNIST, CIFAR-10 for experiments
- scikit-learn toy datasets

**Courses:**
- CS 294: Active Learning (Berkeley)

---

# Bayesian Active Learning

**Bayesian Perspective**: Uncertainty about model parameters $\theta$

**Posterior after seeing data** $\mathcal{D}$:
$$P(\theta | \mathcal{D}) = \frac{P(\mathcal{D} | \theta) P(\theta)}{P(\mathcal{D})}$$

**Predictive distribution**:
$$P(y|x, \mathcal{D}) = \int P(y|x, \theta) P(\theta | \mathcal{D}) d\theta$$

---
**Uncertainty** comes from:
1. **Aleatoric**: Inherent data noise
2. **Epistemic**: Model uncertainty (reducible with more data)

**Active learning targets epistemic uncertainty**

---

# BALD: Bayesian Active Learning by Disagreement

**Goal**: Maximize information gain about model parameters

**Information Gain**:
$$I(y; \theta | x, \mathcal{D}) = H(y|x, \mathcal{D}) - \mathbb{E}_{P(\theta|\mathcal{D})}[H(y|x, \theta)]$$

**Components**:
- $H(y|x, \mathcal{D})$: Entropy of predictive distribution (uncertainty)
- $\mathbb{E}[H(y|x, \theta)]$: Expected conditional entropy (noise)

**Difference** = epistemic uncertainty (what active learning should target)

**Implementation** (with MC Dropout):
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# BatchBALD: BALD for Batches

**Problem**: BALD is for single queries, but we want batches

**Naive approach**: Select top-k BALD scores
**Issue**: May select redundant examples

**BatchBALD**: Maximize joint information gain

**Joint Mutual Information**:
$$I(\{y_1, ..., y_b\}; \theta | \{x_1, ..., x_b\}, \mathcal{D})$$

---
**Greedy approximation**:
1. Select $x_1$ with highest BALD score
2. Select $x_2$ with highest conditional information gain given $x_1$
3. Repeat for batch size $b$

**Implementation** (BatchBALD Acquisition):
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

---

# Expected Error Reduction: Detailed Theory

**Goal**: Select examples that minimize expected future error

**Expected Error** after labeling $(x, y)$:
$$\mathbb{E}_{P(y|x)}[L(\mathcal{D} \cup \{(x,y)\})]$$

where $L(\mathcal{D})$ is loss on validation set given training set $\mathcal{D}$


**Implementation**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---
**Algorithm**:
1. For each unlabeled $x_i$
2. For each possible label $y$:
   - Compute $P(y|x_i)$
   - Simulate adding $(x_i, y)$ to training set
   - Retrain model → compute validation loss
3. Compute expected loss weighted by $P(y|x_i)$
4. Select $x_i$ with minimum expected loss

**Formula**:
$$x^* = \arg\min_{x} \sum_{y} P(y|x) \cdot L(\mathcal{D} \cup \{(x, y)\})$$

---

# Version Space and Active Learning

**Version Space**: Set of all hypotheses consistent with training data

$$VS_{\mathcal{D}} = \{h \in \mathcal{H} : h(x) = y \text{ for all } (x, y) \in \mathcal{D}\}$$

**Query-by-Committee** samples from version space:
- Each committee member represents a hypothesis in $VS$
- Disagreement → large version space → more uncertainty

---
**Optimal query** (theoretically):
- Cuts version space in half
- Maximizes expected reduction in version space size

**Generalized Binary Search**:
$$x^* = \arg\max_{x} \min_{y} |VS_{\mathcal{D} \cup \{(x,y)\}}|$$

**In practice**: Use ensemble to approximate version space

---

# PAC Learning Bounds for Active Learning

**PAC (Probably Approximately Correct)**: With probability $1-\delta$, achieve error $\leq \epsilon$

**Passive learning** label complexity:
$$m_{passive} = O\left(\frac{d \log(1/\epsilon)}{\epsilon}\right)$$

where $d$ = VC dimension

---
**Active learning** label complexity (linear separators):
$$m_{active} = O\left(d \log^2\left(\frac{1}{\epsilon}\right)\right)$$

**Improvement**: Exponential in $\epsilon$ for some hypothesis classes!

**Disagreement coefficient** $\theta$:
$$m_{active} = O\left(\theta d \log\left(\frac{1}{\epsilon}\right)\right)$$

Lower $\theta$ → better active learning performance

---

# Stream-Based Active Learning

**Pool-based**: Have entire unlabeled pool, select best examples
**Stream-based**: Examples arrive sequentially, decide to label or skip

**Decision function**:
$$\text{label}(x) = \begin{cases}
\text{true} & \text{if } U(x) > \tau \\
\text{false} & \text{otherwise}
\end{cases}$$

where $\tau$ is threshold

**Adaptive threshold**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

---

# Membership Query Synthesis

**Standard active learning**: Select from unlabeled pool
**Membership query**: Generate/synthesize queries

**Example** (linear classifier):
- Decision boundary is $w^T x = 0$
- Generate query on decision boundary
- Most informative for refining boundary

**Synthetic query generation**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>



**Challenges**: Synthetic examples may not be realistic or labelable

---

# Multi-Label Active Learning

**Multi-label**: Each example can have multiple labels

**Example**: Image tagging - "beach", "sunset", "people"

**Implementation**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

---
**Uncertainty measures**:

**1. Min-max uncertainty**:
$$U(x) = \max_l P(l|x) - \min_l P(l|x)$$

**2. Avg uncertainty across labels**:
$$U(x) = \frac{1}{L} \sum_{l=1}^{L} H(P(y_l|x))$$

**3. Label cardinality uncertainty**:
$$U(x) = H\left(\sum_{l=1}^{L} P(y_l = 1|x)\right)$$



---

# Active Feature Acquisition

**Scenario**: Features are costly to obtain (medical tests, sensors)

**Goal**: Decide which features AND which examples to acquire

**Decision**:
1. **Example selection**: Which instance to label?
2. **Feature selection**: Which features to measure for that instance?

**Value of information**:
$$VOI(f_i, x) = I(y; f_i | x, \mathcal{D})$$

**Joint optimization**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Active Learning with Label Noise

**Problem**: Oracle labels can be noisy (human errors)

**Strategies**:

**1. Repeated labeling**:
- Label uncertain examples multiple times
- Use majority vote or probabilistic aggregation

---
**2. Confidence-weighted sampling**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

- Ask annotators to provide confidence scores
- Downweight low-confidence labels during training
- Prioritize high-confidence annotations for model updates


**3. Self-consistent active learning**:
- Track annotator consistency
- Weight labels by annotator reliability

---

# Adversarial Active Learning

**Goal**: Robustness to adversarial examples

**Adversarial uncertainty**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

- Create adversarial variants using small input perturbations
- Evaluate model confidence on adversarial inputs
- Large confidence drop indicates model vulnerability
- Query labels for the most adversarially uncertain samples

**Benefit**: Improves robustness to adversarial attacks

---

# Budget-Constrained Active Learning

**Constraint**: Total labeling budget $B$

**Variable costs**: Different samples have different labeling costs

**Optimization**:
$$\max_{\mathcal{S}} \text{Information}(\mathcal{S}) \quad \text{s.t.} \quad \sum_{x \in \mathcal{S}} c(x) \leq B$$

**Cost-aware acquisition**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Regret Bounds for Active Learning

**Regret**: Difference between optimal and actual performance

**Cumulative regret** after $T$ queries:
$$R(T) = \sum_{t=1}^{T} L(h_t) - L(h^*)$$

where:
- $L(h_t)$: Loss of model at iteration $t$
- $L(h^*)$: Loss of optimal model

**Goal**: Minimize regret

---
**Upper bound** (for some algorithms):
$$R(T) = O(\sqrt{T \log T})$$

**Sublinear regret** → algorithm is learning

**In practice**: Track cumulative regret
```python
def compute_regret(accuracies, optimal_accuracy):
    """Compute cumulative regret."""
    losses = 1 - np.array(accuracies)
    optimal_loss = 1 - optimal_accuracy

    regret = losses - optimal_loss
    cumulative_regret = np.cumsum(regret)

    return cumulative_regret
```

---

# Exploration-Exploitation Tradeoff

**Exploration**: Query uncertain examples (learn model)
**Exploitation**: Query high-value examples (improve specific regions)

**Thompson Sampling** for active learning:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

- Train multiple models via bootstrapping
- Randomly sample one model
- Select the most uncertain example under that model
- Naturally balances exploration and exploitation

**Benefit**: Balances exploration and exploitation naturally

---

# Contextual Bandits and Active Learning

**Connection**: Active learning as contextual bandit problem

**Contextual bandit**:
- Context: Unlabeled example $x$
- Actions: Label or skip
- Reward: Information gain

---
**Upper Confidence Bound (UCB)**:
$$\text{Score}(x) = \bar{U}(x) + \sqrt{\frac{2 \log t}{n(x)}}$$

where:
- $\bar{U}(x)$: Average uncertainty for examples similar to $x$
- $n(x)$: Number of times similar examples queried
- $t$: Current iteration

**Implementation**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Active Learning for Structured Prediction

**Structured output**: Sequences, trees, graphs (NER, parsing, segmentation)

**Challenge**: Output space is exponential

**Uncertainty measures**:

**1. Sequence entropy**:
$$H(y|x) = -\sum_{y \in \mathcal{Y}} P(y|x) \log P(y|x)$$

---
**2. Token-level uncertainty** (for sequences):
$$U(x) = \frac{1}{|x|} \sum_{t=1}^{|x|} H(y_t | x)$$

**3. N-best list diversity**:
- Generate top-N predictions
- Measure diversity among top predictions

**Implementation of Structured Active Learning**
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Active Learning for Ranking

**Goal**: Learn ranking function with minimal labeled pairs

**Pairwise comparison**: "Is A better than B?"

**Uncertainty for pairs**:
$$U(x_i, x_j) = |P(x_i \succ x_j) - 0.5|$$

Pairs close to 0.5 are most uncertain

**Query selection**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


**Application**: Search ranking, recommendation systems

---

# Active Learning for Clustering

**Unsupervised active learning**: Query pairwise constraints

**Types of queries**:
1. **Must-link**: Do $x_i$ and $x_j$ belong to same cluster?
2. **Cannot-link**: Do $x_i$ and $x_j$ belong to different clusters?

**Constrained K-means**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


---

# Lifelong Active Learning

**Scenario**: Multiple related tasks over time

**Goal**: Transfer knowledge across tasks to reduce labeling

**Approach**:
1. Learn task $T_1$ with active learning
2. For task $T_2$, use knowledge from $T_1$ to initialize
3. Active learning on $T_2$ with transfer

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


**Benefit**: Faster learning on new related tasks

---

# Online Active Learning

**Setting**: Examples arrive in stream, must decide immediately

**No look-ahead**: Can't compare to future examples

**Challenge**: Balance immediate labeling vs waiting for better example

**Threshold-based online active learning**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>


**Application**: Real-time systems, streaming data

---

# Distributed Active Learning

**Scenario**: Multiple agents/annotators working in parallel

**Challenges**:
1. Coordination: Avoid querying same examples
2. Communication: Share model updates
3. Synchronization: Merge labels and retrain

**Distributed uncertainty sampling**:
<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

**Benefits**: Parallelization, faster labeling

---

# Privacy-Preserving Active Learning

**Goal**: Active learning without exposing sensitive data

**Federated active learning**:
- Data stays on client devices
- Only model updates shared

**Differential privacy**:
- Add noise to queries and labels
- Preserve privacy while learning

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

**Trade-off**: Privacy vs utility

---

# Fairness in Active Learning

**Problem**: Active learning may bias towards majority groups

**Fair active learning**: Ensure diverse coverage across demographic groups

<div style="text-align:center; margin-top:1em;">
  <a href="https://colab.research.google.com/drive/1vyiCXapO4ygeh8apsldzsq-ZDmRPlfSX?usp=sharing"
     style="font-size:1.15em; padding:0.45em 0.9em;
            border:1px solid #444; border-radius:8px;
            text-decoration:none;">
    ▶ Open Class Code on Google Colab
  </a>
</div>

**Benefits**:
- Balanced representation
- Avoid bias amplification
- Improve fairness metrics

---

# Advanced Active Learning Summary

**Theoretical Foundations**:
- Bayesian active learning (BALD, BatchBALD)
- PAC learning bounds
- Version space reduction
- Regret bounds

**Advanced Strategies**:
- Expected error reduction
- Membership query synthesis
- Exploration-exploitation (Thompson sampling, UCB)

---
**Specialized Settings**:
- Multi-label active learning
- Structured prediction
- Ranking and clustering
- Active feature acquisition

**Practical Considerations**:
- Label noise robustness
- Budget constraints
- Variable costs
- Adversarial robustness

---
**Emerging Directions**:
- Lifelong active learning
- Online active learning
- Distributed active learning
- Privacy-preserving active learning
- Fairness in active learning

---

<!-- _class: lead -->
<!-- _paginate: false -->

# Questions?

Next: Data Augmentation
Lab: Build active learning system from scratch
