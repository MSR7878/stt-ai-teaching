{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Lab: Optimizing the Labeling Process\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**  \n",
    "**IIT Gandhinagar**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Implement Active Learning with uncertainty sampling using modAL\n",
    "2. Write and apply labeling functions for Weak Supervision with Snorkel\n",
    "3. Use LLMs (like Gemini) to generate labels programmatically\n",
    "4. Detect and handle noisy labels with cleanlab\n",
    "5. Compare the cost and quality trade-offs of different labeling approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Netflix Movie Theme\n",
    "\n",
    "We need to label 100,000 movie reviews for sentiment analysis. Manual labeling would cost $30,000 and take months. Let's use smarter approaches!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install modAL-python scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install snorkel\n",
    "!pip install cleanlab\n",
    "!pip install google-generativeai  # For Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Create Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic movie review dataset\n",
    "positive_templates = [\n",
    "    \"Amazing movie! Loved every minute of {movie}.\",\n",
    "    \"{movie} was absolutely fantastic. A must-watch!\",\n",
    "    \"Best film I've seen this year. {movie} is a masterpiece.\",\n",
    "    \"Incredible performances in {movie}. Highly recommend!\",\n",
    "    \"{movie} exceeded all my expectations. Perfect 10/10.\",\n",
    "    \"Brilliant storytelling in {movie}. Oscar-worthy!\",\n",
    "    \"Loved {movie}! The acting was superb.\",\n",
    "    \"{movie} is a triumph. Beautifully crafted film.\",\n",
    "]\n",
    "\n",
    "negative_templates = [\n",
    "    \"Terrible movie. {movie} was a complete waste of time.\",\n",
    "    \"{movie} was awful. Avoid at all costs.\",\n",
    "    \"Worst film I've ever seen. {movie} is garbage.\",\n",
    "    \"{movie} was boring and predictable. Very disappointed.\",\n",
    "    \"Don't waste your money on {movie}. Horrible!\",\n",
    "    \"{movie} has no redeeming qualities. Truly bad.\",\n",
    "    \"Painful to watch. {movie} is a disaster.\",\n",
    "    \"{movie} is insulting to audiences. Just awful.\",\n",
    "]\n",
    "\n",
    "neutral_templates = [\n",
    "    \"{movie} was okay. Nothing special.\",\n",
    "    \"Average film. {movie} had its moments.\",\n",
    "    \"{movie} is fine. Not great, not terrible.\",\n",
    "    \"Meh. {movie} was just okay.\",\n",
    "    \"{movie} was watchable but forgettable.\",\n",
    "    \"It's alright. {movie} passes the time.\",\n",
    "]\n",
    "\n",
    "movies = ['Inception', 'The Matrix', 'Avatar', 'Titanic', 'Interstellar', \n",
    "          'The Godfather', 'Pulp Fiction', 'Parasite', 'Joker', 'Gladiator']\n",
    "\n",
    "def generate_reviews(n_samples=1000):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        movie = np.random.choice(movies)\n",
    "        sentiment = np.random.choice(['positive', 'negative', 'neutral'], p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        if sentiment == 'positive':\n",
    "            template = np.random.choice(positive_templates)\n",
    "            label = 1\n",
    "        elif sentiment == 'negative':\n",
    "            template = np.random.choice(negative_templates)\n",
    "            label = 0\n",
    "        else:\n",
    "            template = np.random.choice(neutral_templates)\n",
    "            label = 2  # We'll treat neutral as a separate class\n",
    "        \n",
    "        reviews.append(template.format(movie=movie))\n",
    "        labels.append(label)\n",
    "    \n",
    "    return reviews, labels\n",
    "\n",
    "# Generate dataset\n",
    "reviews, labels = generate_reviews(n_samples=1000)\n",
    "df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "\n",
    "print(f\"Generated {len(df)} reviews\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSample reviews:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to binary classification (positive vs negative, drop neutral for simplicity)\n",
    "df_binary = df[df['label'] != 2].copy()\n",
    "print(f\"Binary dataset size: {len(df_binary)}\")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X = vectorizer.fit_transform(df_binary['review']).toarray()\n",
    "y = df_binary['label'].values\n",
    "\n",
    "# Split into train (unlabeled pool), validation, and test\n",
    "X_pool, X_test, y_pool, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_pool, X_val, y_pool, y_val = train_test_split(X_pool, y_pool, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Pool size: {len(X_pool)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Active Learning with modAL\n",
    "\n",
    "### 3.1 The Active Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling\n",
    "\n",
    "# Start with a small seed set (simulating initial labeled data)\n",
    "n_initial = 20\n",
    "initial_idx = np.random.choice(range(len(X_pool)), size=n_initial, replace=False)\n",
    "\n",
    "X_initial = X_pool[initial_idx]\n",
    "y_initial = y_pool[initial_idx]\n",
    "\n",
    "# Remove initial samples from pool\n",
    "mask = np.ones(len(X_pool), dtype=bool)\n",
    "mask[initial_idx] = False\n",
    "X_pool_remaining = X_pool[mask]\n",
    "y_pool_remaining = y_pool[mask]\n",
    "\n",
    "print(f\"Initial labeled set: {len(X_initial)}\")\n",
    "print(f\"Remaining pool: {len(X_pool_remaining)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): Create Active Learner with Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Create and train Active Learner\n",
    "\n",
    "# Initialize the learner\n",
    "learner = ActiveLearner(\n",
    "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training=X_initial,\n",
    "    y_training=y_initial\n",
    ")\n",
    "\n",
    "# Check initial performance\n",
    "initial_accuracy = learner.score(X_test, y_test)\n",
    "print(f\"Initial accuracy with {n_initial} samples: {initial_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 (Solved): Run Active Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Active Learning Loop\n",
    "\n",
    "n_queries = 50  # Number of samples to query\n",
    "performance_history = [initial_accuracy]\n",
    "\n",
    "# Copy pool to work with\n",
    "X_pool_al = X_pool_remaining.copy()\n",
    "y_pool_al = y_pool_remaining.copy()\n",
    "\n",
    "for i in range(n_queries):\n",
    "    # Query for the most uncertain sample\n",
    "    query_idx, query_instance = learner.query(X_pool_al)\n",
    "    \n",
    "    # Get the label (in practice, this would be a human annotator)\n",
    "    y_new = y_pool_al[query_idx]\n",
    "    \n",
    "    # Teach the model\n",
    "    learner.teach(X_pool_al[query_idx].reshape(1, -1), y_new.reshape(-1))\n",
    "    \n",
    "    # Remove queried sample from pool\n",
    "    X_pool_al = np.delete(X_pool_al, query_idx, axis=0)\n",
    "    y_pool_al = np.delete(y_pool_al, query_idx)\n",
    "    \n",
    "    # Track performance\n",
    "    accuracy = learner.score(X_test, y_test)\n",
    "    performance_history.append(accuracy)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Query {i+1}: Accuracy = {accuracy:.2%}\")\n",
    "\n",
    "print(f\"\\nFinal accuracy: {performance_history[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Compare with Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement random sampling baseline and compare with active learning\n",
    "# Create a learning curve comparing both approaches\n",
    "\n",
    "def random_sampling(classifier, X_pool, n_instances=1):\n",
    "    \"\"\"Random query strategy for baseline comparison.\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Run random sampling loop and collect performance\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4: Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot learning curves for Active Learning vs Random Sampling\n",
    "# X-axis: Number of labeled samples\n",
    "# Y-axis: Test accuracy\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5: Try Different Query Strategies\n",
    "\n",
    "Compare uncertainty sampling, margin sampling, and entropy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare different query strategies\n",
    "# Run active learning with:\n",
    "# 1. uncertainty_sampling\n",
    "# 2. margin_sampling\n",
    "# 3. entropy_sampling\n",
    "# Plot all three on the same graph\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Weak Supervision with Snorkel\n",
    "\n",
    "### 4.1 Understanding Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "# Constants for labels\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Prepare DataFrame for Snorkel\n",
    "df_snorkel = df_binary.copy().reset_index(drop=True)\n",
    "print(f\"Dataset size: {len(df_snorkel)}\")\n",
    "df_snorkel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1 (Solved): Write Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Labeling Functions for Sentiment Analysis\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_amazing(x):\n",
    "    \"\"\"If review contains 'amazing', label as positive.\"\"\"\n",
    "    return POSITIVE if 'amazing' in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_terrible(x):\n",
    "    \"\"\"If review contains 'terrible', label as negative.\"\"\"\n",
    "    return NEGATIVE if 'terrible' in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_loved(x):\n",
    "    \"\"\"If review contains 'loved', label as positive.\"\"\"\n",
    "    return POSITIVE if 'loved' in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_awful(x):\n",
    "    \"\"\"If review contains 'awful', label as negative.\"\"\"\n",
    "    return NEGATIVE if 'awful' in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_masterpiece(x):\n",
    "    \"\"\"If review contains 'masterpiece', label as positive.\"\"\"\n",
    "    return POSITIVE if 'masterpiece' in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_waste(x):\n",
    "    \"\"\"If review contains 'waste', label as negative.\"\"\"\n",
    "    return NEGATIVE if 'waste' in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_exclamation_positive(x):\n",
    "    \"\"\"Multiple exclamations with positive words.\"\"\"\n",
    "    positive_words = ['great', 'amazing', 'love', 'best', 'fantastic']\n",
    "    if x.review.count('!') >= 2:\n",
    "        if any(word in x.review.lower() for word in positive_words):\n",
    "            return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_rating_positive(x):\n",
    "    \"\"\"If mentions high rating (9/10, 10/10), positive.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'(9|10)/10', x.review)\n",
    "    return POSITIVE if match else ABSTAIN\n",
    "\n",
    "# Collect all LFs\n",
    "lfs = [\n",
    "    lf_contains_amazing, lf_contains_terrible, lf_contains_loved,\n",
    "    lf_contains_awful, lf_contains_masterpiece, lf_contains_waste,\n",
    "    lf_exclamation_positive, lf_rating_positive\n",
    "]\n",
    "\n",
    "print(f\"Created {len(lfs)} labeling functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Apply Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LFs to data\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df_snorkel)\n",
    "\n",
    "print(f\"Label matrix shape: {L_train.shape}\")\n",
    "print(f\"Labels: {L_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3 (Solved): Analyze Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Analyze LF performance\n",
    "analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary(Y=df_snorkel['label'].values)\n",
    "print(\"Labeling Function Analysis:\")\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4: Train Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the Label Model and get probabilistic labels\n",
    "\n",
    "# Your code here\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "# Train the model\n",
    "# Get predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.5: Write More Labeling Functions\n",
    "\n",
    "Create 3 additional labeling functions to improve coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write 3 more labeling functions\n",
    "# Consider: word patterns, punctuation, sentence structure\n",
    "\n",
    "@labeling_function()\n",
    "def lf_your_function_1(x):\n",
    "    \"\"\"Description\"\"\"\n",
    "    # Your code here\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_your_function_2(x):\n",
    "    \"\"\"Description\"\"\"\n",
    "    # Your code here\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_your_function_3(x):\n",
    "    \"\"\"Description\"\"\"\n",
    "    # Your code here\n",
    "    return ABSTAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.6: Train Downstream Model with Weak Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a classifier using the probabilistic labels from the Label Model\n",
    "# Compare performance with a model trained on true labels\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: LLM-Based Labeling\n",
    "\n",
    "### 5.1 Setting Up Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API key (get from https://makersuite.google.com/app/apikey)\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    client = genai.Client(api_key=os.environ.get('GEMINI_API_KEY', ''))\n",
    "    print(\"Gemini client initialized!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Set GEMINI_API_KEY environment variable to use LLM labeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (Solved): Create LLM Labeling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: LLM Labeling Function (mock implementation for demo)\n",
    "\n",
    "def label_with_llm(review, use_api=False):\n",
    "    \"\"\"\n",
    "    Label a movie review using an LLM.\n",
    "    \n",
    "    Args:\n",
    "        review: The review text\n",
    "        use_api: If True, call actual API; else use mock\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'label' and 'confidence'\n",
    "    \"\"\"\n",
    "    if use_api and 'GEMINI_API_KEY' in os.environ:\n",
    "        prompt = f\"\"\"\n",
    "        Classify the following movie review as POSITIVE or NEGATIVE.\n",
    "        \n",
    "        Review: \"{review}\"\n",
    "        \n",
    "        Respond with JSON format:\n",
    "        {{\"label\": \"POSITIVE\" or \"NEGATIVE\", \"confidence\": 0.0-1.0}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            contents=prompt\n",
    "        )\n",
    "        \n",
    "        import json\n",
    "        result = json.loads(response.text)\n",
    "        return result\n",
    "    else:\n",
    "        # Mock implementation based on keywords\n",
    "        positive_words = ['amazing', 'loved', 'fantastic', 'masterpiece', 'best', 'incredible']\n",
    "        negative_words = ['terrible', 'awful', 'worst', 'waste', 'boring', 'disaster']\n",
    "        \n",
    "        review_lower = review.lower()\n",
    "        pos_count = sum(1 for w in positive_words if w in review_lower)\n",
    "        neg_count = sum(1 for w in negative_words if w in review_lower)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return {'label': 'POSITIVE', 'confidence': 0.8 + 0.1 * pos_count}\n",
    "        elif neg_count > pos_count:\n",
    "            return {'label': 'NEGATIVE', 'confidence': 0.8 + 0.1 * neg_count}\n",
    "        else:\n",
    "            return {'label': 'POSITIVE', 'confidence': 0.5}  # Default\n",
    "\n",
    "# Test\n",
    "test_review = \"Amazing movie! Loved every minute of Inception.\"\n",
    "result = label_with_llm(test_review)\n",
    "print(f\"Review: {test_review}\")\n",
    "print(f\"Label: {result['label']}, Confidence: {result['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Batch Label with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Label a batch of reviews using the LLM function\n",
    "# Track time and estimated cost\n",
    "\n",
    "sample_reviews = df_binary['review'].head(20).tolist()\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3: Evaluate LLM Labels Against Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate accuracy, precision, recall of LLM labels\n",
    "# Compare with human labels (ground truth)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4: Hybrid Labeling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement hybrid labeling\n",
    "# - Use LLM for high-confidence labels\n",
    "# - Send low-confidence examples to human review\n",
    "\n",
    "def hybrid_labeling(reviews, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Label reviews using hybrid approach.\n",
    "    \n",
    "    Returns:\n",
    "        llm_labeled: Reviews labeled by LLM\n",
    "        human_queue: Reviews needing human review\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Noisy Label Detection with cleanlab\n",
    "\n",
    "### 6.1 Setting Up cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab import Datalab\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Create a dataset with some noisy labels\n",
    "X_noisy = X.copy()\n",
    "y_noisy = y.copy()\n",
    "\n",
    "# Flip 10% of labels to simulate noise\n",
    "noise_idx = np.random.choice(len(y_noisy), size=int(0.1 * len(y_noisy)), replace=False)\n",
    "y_noisy[noise_idx] = 1 - y_noisy[noise_idx]  # Flip labels\n",
    "\n",
    "print(f\"Added noise to {len(noise_idx)} labels ({len(noise_idx)/len(y_noisy):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1 (Solved): Find Label Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Use cleanlab to find label issues\n",
    "\n",
    "# Get out-of-fold predictions\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "pred_probs = cross_val_predict(clf, X_noisy, y_noisy, cv=5, method='predict_proba')\n",
    "\n",
    "# Create Datalab and find issues\n",
    "data_dict = {\n",
    "    'labels': y_noisy\n",
    "}\n",
    "\n",
    "lab = Datalab(data=data_dict, label_name='labels')\n",
    "lab.find_issues(pred_probs=pred_probs)\n",
    "\n",
    "# Get summary\n",
    "print(\"Issue Summary:\")\n",
    "print(lab.get_issue_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2: Analyze Detected Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed issues\n",
    "issues = lab.get_issues()\n",
    "print(\"\\nLabel Issues:\")\n",
    "label_issues = issues[issues['is_label_issue'] == True]\n",
    "print(f\"Found {len(label_issues)} potential label errors\")\n",
    "\n",
    "# TODO: Calculate precision/recall of cleanlab's detection\n",
    "# How many of the actual noisy labels did it find?\n",
    "# How many false positives?\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.3: Train on Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare model performance:\n",
    "# 1. Trained on all data (with noise)\n",
    "# 2. Trained on clean data (removing detected issues)\n",
    "# 3. Trained on true clean data (oracle)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Cost Comparison\n",
    "\n",
    "### Question 7.1: Calculate Labeling Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and compare costs of different approaches\n",
    "\n",
    "def calculate_costs(n_items, method):\n",
    "    \"\"\"\n",
    "    Calculate estimated cost for different labeling methods.\n",
    "    \n",
    "    Args:\n",
    "        n_items: Number of items to label\n",
    "        method: 'manual', 'active_learning', 'weak_supervision', 'llm', 'hybrid'\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost, time, quality estimates\n",
    "    \"\"\"\n",
    "    costs = {\n",
    "        'manual': {\n",
    "            'cost_per_item': 0.30,  # USD\n",
    "            'time_per_item': 0.1,   # minutes\n",
    "            'quality': 0.95,        # accuracy\n",
    "            'items_needed': n_items\n",
    "        },\n",
    "        'active_learning': {\n",
    "            'cost_per_item': 0.30,\n",
    "            'time_per_item': 0.1,\n",
    "            'quality': 0.90,\n",
    "            'items_needed': n_items // 3  # 3x reduction\n",
    "        },\n",
    "        'weak_supervision': {\n",
    "            'cost_per_item': 0.0,   # Just engineer time\n",
    "            'setup_cost': 500,      # Writing LFs\n",
    "            'time_per_item': 0.0,\n",
    "            'quality': 0.80,\n",
    "            'items_needed': 0\n",
    "        },\n",
    "        'llm': {\n",
    "            'cost_per_item': 0.002,  # API cost\n",
    "            'time_per_item': 0.01,   # minutes\n",
    "            'quality': 0.85,\n",
    "            'items_needed': n_items\n",
    "        },\n",
    "        'hybrid': {\n",
    "            'cost_per_item': 0.05,  # Average\n",
    "            'time_per_item': 0.02,\n",
    "            'quality': 0.90,\n",
    "            'items_needed': n_items\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    method_info = costs[method]\n",
    "    total_cost = method_info['cost_per_item'] * method_info['items_needed']\n",
    "    if 'setup_cost' in method_info:\n",
    "        total_cost += method_info['setup_cost']\n",
    "    total_time = method_info['time_per_item'] * method_info['items_needed']\n",
    "    \n",
    "    return {\n",
    "        'method': method,\n",
    "        'total_cost': total_cost,\n",
    "        'total_time_hours': total_time / 60,\n",
    "        'expected_quality': method_info['quality']\n",
    "    }\n",
    "\n",
    "# Compare all methods for 100,000 items\n",
    "n_items = 100000\n",
    "methods = ['manual', 'active_learning', 'weak_supervision', 'llm', 'hybrid']\n",
    "\n",
    "print(f\"Cost comparison for {n_items:,} items:\\n\")\n",
    "print(f\"{'Method':<20} {'Cost ($)':<12} {'Time (hrs)':<12} {'Quality':<10}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "for method in methods:\n",
    "    result = calculate_costs(n_items, method)\n",
    "    print(f\"{result['method']:<20} ${result['total_cost']:>10,.2f} {result['total_time_hours']:>10,.1f} {result['expected_quality']:>8.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems\n",
    "\n",
    "### Challenge 1: Batch Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement batch active learning with diversity\n",
    "# Query 10 samples at a time, ensuring diversity using clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def diversity_uncertainty_sampling(classifier, X_pool, n_instances=10):\n",
    "    \"\"\"\n",
    "    Select n_instances that are both uncertain AND diverse.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Get uncertainty scores for all samples\n",
    "    2. Cluster samples into n_instances clusters\n",
    "    3. Pick most uncertain sample from each cluster\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Combine All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Build a complete labeling pipeline that:\n",
    "# 1. Uses weak supervision for initial bulk labels\n",
    "# 2. Uses LLM to label uncertain examples\n",
    "# 3. Uses active learning for remaining hard cases\n",
    "# 4. Uses cleanlab to detect and fix errors\n",
    "\n",
    "class SmartLabelingPipeline:\n",
    "    def __init__(self, labeling_functions, llm_client=None, confidence_threshold=0.8):\n",
    "        self.lfs = labeling_functions\n",
    "        self.llm_client = llm_client\n",
    "        self.threshold = confidence_threshold\n",
    "    \n",
    "    def label(self, data):\n",
    "        \"\"\"\n",
    "        Label data using the hybrid pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            labels: Final labels\n",
    "            sources: Source of each label (weak/llm/human)\n",
    "            confidence: Confidence scores\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Active Learning with Human-in-the-Loop UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Create an interactive labeling interface\n",
    "# Use ipywidgets to create a simple annotation UI\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Create interactive labeling widget\n",
    "    # Your code here\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Install ipywidgets for interactive labeling: pip install ipywidgets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Active Learning**: Query strategies (uncertainty, margin, entropy) to label smarter\n",
    "2. **Weak Supervision**: Writing labeling functions to generate noisy but cheap labels\n",
    "3. **LLM Labeling**: Using GPT/Gemini as automated annotators\n",
    "4. **Noisy Label Detection**: Using cleanlab to find and fix label errors\n",
    "5. **Cost Comparison**: Trade-offs between different labeling approaches\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Method | Best When | Typical Savings |\n",
    "|--------|-----------|----------------|\n",
    "| Active Learning | Limited budget | 2-10x |\n",
    "| Weak Supervision | Patterns exist | 10-100x |\n",
    "| LLM Labeling | Well-defined task | 10-50x |\n",
    "| Hybrid | Large scale projects | 5-20x |\n",
    "\n",
    "### Next Week\n",
    "\n",
    "Week 5: Data Augmentation - Create more training data without any labeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
