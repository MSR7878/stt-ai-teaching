{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Lab: Model Optimization for Edge Deployment\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will learn to:\n",
    "1. **Quantize models** to reduce size by 4x\n",
    "2. **Prune models** to remove redundant weights\n",
    "3. **Export to ONNX** for cross-platform deployment\n",
    "4. **Benchmark** latency and accuracy trade-offs\n",
    "\n",
    "**Goal**: Optimize a model for deployment on resource-constrained devices.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision onnx onnxruntime numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Baseline Measurement\n",
    "\n",
    "Before optimizing, we need to measure what we're starting with.\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│                  Optimization Pipeline                   │\n",
    "│                                                          │\n",
    "│  Original      Quantize     Prune       ONNX Export     │\n",
    "│   Model   ──►  (4x smaller) ──►  (faster) ──►  (portable)│\n",
    "│                                                          │\n",
    "│  45 MB    ──►   11 MB      ──►   8 MB    ──►   Deploy!  │\n",
    "│                                                          │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 (Solved): Load and Measure Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "# Load pre-trained ResNet-18\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Save model to measure size\n",
    "torch.save(model.state_dict(), 'resnet18_fp32.pth')\n",
    "size_mb = os.path.getsize('resnet18_fp32.pth') / (1024 * 1024)\n",
    "print(f\"Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (Solved): Measure Inference Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "def benchmark_model(model, input_shape=(1, 3, 224, 224), num_runs=100):\n",
    "    \"\"\"Benchmark model inference latency.\"\"\"\n",
    "    input_data = torch.randn(input_shape)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_data)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_data)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        'mean_ms': np.mean(times) * 1000,\n",
    "        'std_ms': np.std(times) * 1000,\n",
    "        'p95_ms': np.percentile(times, 95) * 1000\n",
    "    }\n",
    "\n",
    "# Run benchmark\n",
    "baseline_metrics = benchmark_model(model)\n",
    "print(f\"Mean latency: {baseline_metrics['mean_ms']:.2f} ms\")\n",
    "print(f\"Std latency: {baseline_metrics['std_ms']:.2f} ms\")\n",
    "print(f\"P95 latency: {baseline_metrics['p95_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Record Your Baseline\n",
    "\n",
    "Fill in your baseline measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR BASELINE MEASUREMENTS\n",
    "baseline = {\n",
    "    'model_size_mb': None,      # Fill in from above\n",
    "    'total_params': None,       # Fill in from above\n",
    "    'mean_latency_ms': None,    # Fill in from above\n",
    "    'p95_latency_ms': None      # Fill in from above\n",
    "}\n",
    "\n",
    "print(\"Baseline metrics:\")\n",
    "for key, value in baseline.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Quantization\n",
    "\n",
    "Reduce precision from FP32 to INT8 for 4x smaller models.\n",
    "\n",
    "## 2.1 Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (Solved): Apply Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "# Load fresh model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear},  # Layers to quantize\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save and measure size\n",
    "torch.save(quantized_model.state_dict(), 'resnet18_int8_dynamic.pth')\n",
    "quant_size_mb = os.path.getsize('resnet18_int8_dynamic.pth') / (1024 * 1024)\n",
    "\n",
    "print(f\"Quantized model size: {quant_size_mb:.2f} MB\")\n",
    "print(f\"Compression ratio: {size_mb / quant_size_mb:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Benchmark Quantized Model\n",
    "\n",
    "Measure the latency of the quantized model and compare to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Benchmark the quantized model\n",
    "\n",
    "# Print comparison with baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Static Quantization (Bonus)\n",
    "\n",
    "Static quantization uses calibration data for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Implement static quantization\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Pruning\n",
    "\n",
    "Remove redundant weights to make the model smaller and faster.\n",
    "\n",
    "## 3.1 Unstructured Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): Apply Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Load fresh model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Prune 30% of weights in all Conv2d layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
    "\n",
    "# Make pruning permanent\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "# Count zero weights (sparsity)\n",
    "total_zeros = 0\n",
    "total_params = 0\n",
    "\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "    total_zeros += (param == 0).sum().item()\n",
    "\n",
    "sparsity = 100 * total_zeros / total_params\n",
    "print(f\"Global sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Benchmark Pruned Model\n",
    "\n",
    "Measure size and latency of the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Structured Pruning\n",
    "\n",
    "Try structured pruning (removing entire filters) with 20% pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: ONNX Export\n",
    "\n",
    "Export models to ONNX format for cross-platform deployment.\n",
    "\n",
    "## 4.1 Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1 (Solved): Export Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import onnx\n",
    "\n",
    "# Load model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"resnet18.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Model exported to resnet18.onnx\")\n",
    "\n",
    "# Verify the model\n",
    "onnx_model = onnx.load(\"resnet18.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Run Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"resnet18.onnx\")\n",
    "\n",
    "# Prepare input\n",
    "input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {'input': input_data})\n",
    "\n",
    "print(f\"Output shape: {outputs[0].shape}\")\n",
    "print(f\"Predicted class: {np.argmax(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3: Benchmark ONNX Runtime\n",
    "\n",
    "Compare ONNX Runtime performance to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def benchmark_onnx(session, input_shape=(1, 3, 224, 224), num_runs=100):\n",
    "    \"\"\"Benchmark ONNX Runtime inference.\"\"\"\n",
    "    pass  # Implement this\n",
    "\n",
    "# Run benchmark and compare to PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Comprehensive Comparison\n",
    "\n",
    "Compare all optimization methods.\n",
    "\n",
    "## 5.1 Create Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1: Fill in Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a comparison table with your measurements\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    'Model': ['Baseline FP32', 'Dynamic INT8', 'Pruned 30%', 'ONNX FP32', 'ONNX INT8'],\n",
    "    'Size (MB)': [None, None, None, None, None],  # Fill in\n",
    "    'Latency (ms)': [None, None, None, None, None],  # Fill in\n",
    "    'Compression': ['1x', None, None, None, None],  # Fill in\n",
    "    'Speedup': ['1x', None, None, None, None]  # Fill in\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Create Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create bar charts comparing:\n",
    "# 1. Model sizes\n",
    "# 2. Inference latencies\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Add your visualization code\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Accuracy Evaluation\n",
    "\n",
    "Optimization shouldn't hurt accuracy too much.\n",
    "\n",
    "## 6.1 Test on Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1: Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that optimized models give similar predictions\n",
    "\n",
    "# Load all models\n",
    "baseline_model = models.resnet18(pretrained=True)\n",
    "baseline_model.eval()\n",
    "\n",
    "# Create test input\n",
    "test_input = torch.randn(10, 3, 224, 224)\n",
    "\n",
    "# Get baseline predictions\n",
    "with torch.no_grad():\n",
    "    baseline_preds = baseline_model(test_input).argmax(dim=1)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Compare predictions from:\n",
    "# 1. Quantized model\n",
    "# 2. ONNX model\n",
    "# Calculate agreement rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Baseline measurement**: Size, latency, parameters\n",
    "2. **Quantization**: Dynamic and static quantization (4x smaller)\n",
    "3. **Pruning**: Removing weights for sparsity\n",
    "4. **ONNX**: Cross-platform model export\n",
    "5. **Benchmarking**: Fair comparison methods\n",
    "\n",
    "## Typical Results\n",
    "\n",
    "| Optimization | Size Reduction | Speedup | Accuracy Loss |\n",
    "|--------------|----------------|---------|---------------|\n",
    "| Quantization | 4x | 2-3x | <1% |\n",
    "| Pruning | 1.5-2x | Variable | 1-3% |\n",
    "| ONNX | Same | 2-3x | 0% |\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit:\n",
    "1. This completed notebook\n",
    "2. Your comparison chart (`optimization_comparison.png`)\n",
    "3. Brief report (1 page): Which optimization would you use for a mobile app?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
