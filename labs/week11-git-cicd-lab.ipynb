{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Lab: Git, Testing & CI/CD\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will learn to:\n",
    "1. **Write tests** with pytest\n",
    "2. **Automate testing** with GitHub Actions\n",
    "3. **Build CI/CD pipelines** for ML projects\n",
    "4. **Create GitHub bots** using PyGithub\n",
    "\n",
    "**Goal**: Set up automated testing and deployment for an ML project.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pytest pytest-cov PyGithub scikit-learn numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Testing Basics with pytest\n",
    "\n",
    "Good tests make your code reliable.\n",
    "\n",
    "## 1.1 Your First Tests\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     Testing Pyramid                    â”‚\n",
    "â”‚                                                        â”‚\n",
    "â”‚                        /\\                              â”‚\n",
    "â”‚                       /  \\                             â”‚\n",
    "â”‚                      / E2E\\  (End-to-End)              â”‚\n",
    "â”‚                     /â”€â”€â”€â”€â”€â”€\\                           â”‚\n",
    "â”‚                    /        \\                          â”‚\n",
    "â”‚                   /Integration\\                        â”‚\n",
    "â”‚                  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\                      â”‚\n",
    "â”‚                 /                \\                     â”‚\n",
    "â”‚                /    Unit Tests    \\  (Most tests)      â”‚\n",
    "â”‚               /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\                   â”‚\n",
    "â”‚                                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 (Solved): Create a Simple Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "# Create a project structure\n",
    "os.makedirs(\"my_ml_project/src\", exist_ok=True)\n",
    "os.makedirs(\"my_ml_project/tests\", exist_ok=True)\n",
    "\n",
    "# Create a simple utility module\n",
    "utils_code = '''\n",
    "\"\"\"Utility functions for data processing.\"\"\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by lowercasing and stripping whitespace.\"\"\"\n",
    "    if text is None:\n",
    "        raise ValueError(\"Text cannot be None\")\n",
    "    return text.lower().strip()\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy score.\"\"\"\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Length mismatch\")\n",
    "    if len(y_true) == 0:\n",
    "        raise ValueError(\"Empty arrays\")\n",
    "    \n",
    "    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def validate_features(features, expected_count):\n",
    "    \"\"\"Validate feature array.\"\"\"\n",
    "    if not isinstance(features, (list, tuple)):\n",
    "        raise TypeError(\"Features must be a list or tuple\")\n",
    "    if len(features) != expected_count:\n",
    "        raise ValueError(f\"Expected {expected_count} features, got {len(features)}\")\n",
    "    return True\n",
    "'''\n",
    "\n",
    "with open(\"my_ml_project/src/utils.py\", \"w\") as f:\n",
    "    f.write(utils_code)\n",
    "\n",
    "# Create __init__.py\n",
    "with open(\"my_ml_project/src/__init__.py\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "print(\"Created my_ml_project/src/utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (Solved): Write Basic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "test_code = '''\n",
    "\"\"\"Tests for utility functions.\"\"\"\n",
    "import pytest\n",
    "import sys\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from utils import normalize_text, calculate_accuracy, validate_features\n",
    "\n",
    "# Test normalize_text\n",
    "def test_normalize_text_basic():\n",
    "    \"\"\"Test basic text normalization.\"\"\"\n",
    "    assert normalize_text(\"  Hello World  \") == \"hello world\"\n",
    "\n",
    "def test_normalize_text_already_clean():\n",
    "    \"\"\"Test with already clean text.\"\"\"\n",
    "    assert normalize_text(\"hello\") == \"hello\"\n",
    "\n",
    "def test_normalize_text_none_raises():\n",
    "    \"\"\"Test that None input raises ValueError.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        normalize_text(None)\n",
    "\n",
    "# Test calculate_accuracy\n",
    "def test_accuracy_perfect():\n",
    "    \"\"\"Test 100% accuracy.\"\"\"\n",
    "    assert calculate_accuracy([1, 2, 3], [1, 2, 3]) == 1.0\n",
    "\n",
    "def test_accuracy_zero():\n",
    "    \"\"\"Test 0% accuracy.\"\"\"\n",
    "    assert calculate_accuracy([1, 1, 1], [0, 0, 0]) == 0.0\n",
    "\n",
    "def test_accuracy_partial():\n",
    "    \"\"\"Test partial accuracy.\"\"\"\n",
    "    assert calculate_accuracy([1, 1, 0, 0], [1, 0, 0, 0]) == 0.75\n",
    "\n",
    "def test_accuracy_length_mismatch():\n",
    "    \"\"\"Test that mismatched lengths raise error.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_accuracy([1, 2], [1])\n",
    "'''\n",
    "\n",
    "with open(\"my_ml_project/tests/test_utils.py\", \"w\") as f:\n",
    "    f.write(test_code)\n",
    "\n",
    "with open(\"my_ml_project/tests/__init__.py\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "print(\"Created my_ml_project/tests/test_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Run the Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pytest\n",
    "!cd my_ml_project && python -m pytest tests/ -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4: Add More Tests\n",
    "\n",
    "Write additional tests for `validate_features`:\n",
    "1. Test valid input (list of 4 features)\n",
    "2. Test wrong count (5 features when expecting 4)\n",
    "3. Test wrong type (string instead of list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "additional_tests = '''\n",
    "# Add your tests for validate_features here\n",
    "\n",
    "'''\n",
    "\n",
    "# Append to test file\n",
    "# with open(\"my_ml_project/tests/test_utils.py\", \"a\") as f:\n",
    "#     f.write(additional_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Testing ML Models\n",
    "\n",
    "ML models need special testing strategies.\n",
    "\n",
    "## 2.1 Model Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (Solved): Test Model Can Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "model_test_code = '''\n",
    "\"\"\"Tests for ML models.\"\"\"\n",
    "import pytest\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "@pytest.fixture\n",
    "def small_dataset():\n",
    "    \"\"\"Create a small dataset for testing.\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=100,\n",
    "        n_features=10,\n",
    "        n_informative=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "@pytest.fixture\n",
    "def trained_model(small_dataset):\n",
    "    \"\"\"Return a trained model.\"\"\"\n",
    "    X, y = small_dataset\n",
    "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def test_model_can_overfit(small_dataset):\n",
    "    \"\"\"Test that model can achieve high accuracy on training data.\"\"\"\n",
    "    X, y = small_dataset\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    train_accuracy = model.score(X, y)\n",
    "    assert train_accuracy > 0.95, f\"Model should overfit, got {train_accuracy}\"\n",
    "\n",
    "def test_model_predictions_shape(trained_model, small_dataset):\n",
    "    \"\"\"Test that predictions have correct shape.\"\"\"\n",
    "    X, _ = small_dataset\n",
    "    predictions = trained_model.predict(X)\n",
    "    \n",
    "    assert predictions.shape == (100,), f\"Expected (100,), got {predictions.shape}\"\n",
    "\n",
    "def test_model_probabilities_sum_to_one(trained_model, small_dataset):\n",
    "    \"\"\"Test that class probabilities sum to 1.\"\"\"\n",
    "    X, _ = small_dataset\n",
    "    probas = trained_model.predict_proba(X)\n",
    "    \n",
    "    row_sums = probas.sum(axis=1)\n",
    "    assert np.allclose(row_sums, 1.0), \"Probabilities should sum to 1\"\n",
    "\n",
    "def test_model_handles_single_sample(trained_model):\n",
    "    \"\"\"Test prediction on a single sample.\"\"\"\n",
    "    single_sample = np.random.randn(1, 10)\n",
    "    prediction = trained_model.predict(single_sample)\n",
    "    \n",
    "    assert len(prediction) == 1\n",
    "'''\n",
    "\n",
    "with open(\"my_ml_project/tests/test_model.py\", \"w\") as f:\n",
    "    f.write(model_test_code)\n",
    "\n",
    "print(\"Created my_ml_project/tests/test_model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model tests\n",
    "!cd my_ml_project && python -m pytest tests/test_model.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Write Invariance Tests\n",
    "\n",
    "Write tests that check model invariances:\n",
    "1. Same input should give same output (determinism)\n",
    "2. Predictions should be in valid range (0 or 1 for binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: GitHub Actions CI/CD\n",
    "\n",
    "Automate testing with GitHub Actions.\n",
    "\n",
    "## 3.1 Creating a Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): Create CI Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "os.makedirs(\"my_ml_project/.github/workflows\", exist_ok=True)\n",
    "\n",
    "ci_workflow = '''\n",
    "name: CI Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main, master]\n",
    "  pull_request:\n",
    "    branches: [main, master]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v4\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v5\n",
    "      with:\n",
    "        python-version: \"3.10\"\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest pytest-cov\n",
    "    \n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest tests/ -v --cov=src --cov-report=xml\n",
    "    \n",
    "    - name: Upload coverage\n",
    "      uses: codecov/codecov-action@v3\n",
    "      with:\n",
    "        files: ./coverage.xml\n",
    "'''\n",
    "\n",
    "with open(\"my_ml_project/.github/workflows/ci.yml\", \"w\") as f:\n",
    "    f.write(ci_workflow)\n",
    "\n",
    "print(\"Created .github/workflows/ci.yml\")\n",
    "print(\"\\nThis workflow will:\")\n",
    "print(\"1. Trigger on push/PR to main branch\")\n",
    "print(\"2. Set up Python 3.10\")\n",
    "print(\"3. Install dependencies\")\n",
    "print(\"4. Run tests with coverage\")\n",
    "print(\"5. Upload coverage report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Create Requirements File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt\n",
    "requirements = \"\"\"numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "scikit-learn>=1.3.0\n",
    "pytest>=7.4.0\n",
    "pytest-cov>=4.1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"my_ml_project/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Created requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Add Linting to CI\n",
    "\n",
    "Add a linting step to the CI workflow using `ruff` or `flake8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Modify the workflow to add linting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: GitHub Automation with PyGithub\n",
    "\n",
    "Automate GitHub tasks with Python.\n",
    "\n",
    "## 4.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1 (Solved): Connect to GitHub\n",
    "\n",
    "**Note**: You need a GitHub Personal Access Token.\n",
    "\n",
    "Get one from: https://github.com/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "from github import Github\n",
    "import os\n",
    "\n",
    "# Set your token (in real code, use environment variables!)\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", \"your-token-here\")\n",
    "\n",
    "# Connect to GitHub\n",
    "g = Github(GITHUB_TOKEN)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    user = g.get_user()\n",
    "    print(f\"Connected as: {user.login}\")\n",
    "    print(f\"Public repos: {user.public_repos}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    print(\"Make sure to set your GITHUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2 (Solved): List Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "def list_repos(github_client, limit=5):\n",
    "    \"\"\"List user repositories.\"\"\"\n",
    "    user = github_client.get_user()\n",
    "    repos = list(user.get_repos()[:limit])\n",
    "    \n",
    "    print(f\"\\nYour first {limit} repositories:\")\n",
    "    for repo in repos:\n",
    "        print(f\"  - {repo.name}: {repo.stargazers_count} stars\")\n",
    "    \n",
    "    return repos\n",
    "\n",
    "# Uncomment to run (requires valid token)\n",
    "# list_repos(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3: Create Issue Automatically\n",
    "\n",
    "Write a function that creates an issue when tests fail:\n",
    "\n",
    "```python\n",
    "def create_failure_issue(repo_name, test_name, error_message):\n",
    "    \"\"\"Create a GitHub issue for test failure.\"\"\"\n",
    "    # Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Build an AI PR Reviewer\n",
    "\n",
    "Create a bot that reviews PRs using an LLM.\n",
    "\n",
    "## 5.1 The Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (Solved): PR Review Bot Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "review_bot_code = '''\n",
    "\"\"\"AI-powered PR Review Bot.\"\"\"\n",
    "from github import Github\n",
    "import os\n",
    "\n",
    "# For LLM integration (example with OpenAI)\n",
    "# import openai\n",
    "\n",
    "class PRReviewBot:\n",
    "    def __init__(self, github_token):\n",
    "        self.github = Github(github_token)\n",
    "    \n",
    "    def get_pr_diff(self, repo_name, pr_number):\n",
    "        \"\"\"Get the diff of a pull request.\"\"\"\n",
    "        repo = self.github.get_repo(repo_name)\n",
    "        pr = repo.get_pull(pr_number)\n",
    "        \n",
    "        files_changed = []\n",
    "        for file in pr.get_files():\n",
    "            files_changed.append({\n",
    "                \"filename\": file.filename,\n",
    "                \"status\": file.status,\n",
    "                \"patch\": file.patch\n",
    "            })\n",
    "        \n",
    "        return files_changed\n",
    "    \n",
    "    def review_with_llm(self, diff):\n",
    "        \"\"\"Send diff to LLM for review (placeholder).\"\"\"\n",
    "        # In real implementation, call OpenAI/Gemini API\n",
    "        prompt = f\"\"\"\n",
    "        Review this code change for:\n",
    "        1. Bugs or errors\n",
    "        2. Security issues\n",
    "        3. Code style problems\n",
    "        4. Performance concerns\n",
    "        \n",
    "        Code diff:\n",
    "        {diff}\n",
    "        \n",
    "        Provide a brief, helpful review.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Placeholder response\n",
    "        return \"This is where the LLM review would go.\"\n",
    "    \n",
    "    def post_review_comment(self, repo_name, pr_number, comment):\n",
    "        \"\"\"Post a review comment on the PR.\"\"\"\n",
    "        repo = self.github.get_repo(repo_name)\n",
    "        pr = repo.get_pull(pr_number)\n",
    "        \n",
    "        pr.create_issue_comment(f\"ðŸ¤– **AI Review Bot**\\n\\n{comment}\")\n",
    "        print(f\"Posted review on PR #{pr_number}\")\n",
    "\n",
    "# Usage example\n",
    "# bot = PRReviewBot(os.getenv(\"GITHUB_TOKEN\"))\n",
    "# diff = bot.get_pr_diff(\"owner/repo\", 123)\n",
    "# review = bot.review_with_llm(diff)\n",
    "# bot.post_review_comment(\"owner/repo\", 123, review)\n",
    "'''\n",
    "\n",
    "with open(\"my_ml_project/review_bot.py\", \"w\") as f:\n",
    "    f.write(review_bot_code)\n",
    "\n",
    "print(\"Created review_bot.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Add LLM Integration\n",
    "\n",
    "Complete the `review_with_llm` method using either:\n",
    "- OpenAI API\n",
    "- Google Gemini API\n",
    "- Anthropic Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Complete the LLM integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Code Coverage\n",
    "\n",
    "Measure how much of your code is tested.\n",
    "\n",
    "## 6.1 Running Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests with coverage\n",
    "!cd my_ml_project && python -m pytest tests/ -v --cov=src --cov-report=term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1: Improve Coverage\n",
    "\n",
    "Look at the coverage report. Which functions are not fully covered?\n",
    "\n",
    "Write tests to improve coverage to at least 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **pytest basics**: Writing tests, fixtures, assertions\n",
    "2. **ML testing**: Model tests, invariance tests\n",
    "3. **GitHub Actions**: CI/CD workflows\n",
    "4. **PyGithub**: Automating GitHub tasks\n",
    "5. **Code coverage**: Measuring test coverage\n",
    "\n",
    "## Files Created\n",
    "\n",
    "```\n",
    "my_ml_project/\n",
    "â”œâ”€â”€ .github/\n",
    "â”‚   â””â”€â”€ workflows/\n",
    "â”‚       â””â”€â”€ ci.yml\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â””â”€â”€ utils.py\n",
    "â”œâ”€â”€ tests/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ test_utils.py\n",
    "â”‚   â””â”€â”€ test_model.py\n",
    "â”œâ”€â”€ review_bot.py\n",
    "â””â”€â”€ requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit:\n",
    "1. This completed notebook\n",
    "2. Screenshot of GitHub Actions passing\n",
    "3. Coverage report showing >80% coverage\n",
    "4. Your `review_bot.py` with LLM integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
