{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Lab: Data Augmentation\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**  \n",
    "**IIT Gandhinagar**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Apply image augmentation techniques using Albumentations\n",
    "2. Perform text augmentation using nlpaug\n",
    "3. Understand audio augmentation with audiomentations\n",
    "4. Design appropriate augmentation pipelines for different tasks\n",
    "5. Measure the impact of augmentation on model performance\n",
    "\n",
    "---\n",
    "\n",
    "## Netflix Movie Theme\n",
    "\n",
    "We'll augment our movie data to improve model performance:\n",
    "- **Movie posters**: Image augmentation for genre classification\n",
    "- **Movie reviews**: Text augmentation for sentiment analysis\n",
    "- **Audio clips**: Audio augmentation for trailer classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install albumentations opencv-python-headless pillow\n",
    "!pip install nlpaug transformers\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Image Augmentation with Albumentations\n",
    "\n",
    "### 2.1 Creating Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image (movie poster placeholder)\n",
    "def create_sample_poster(width=400, height=600):\n",
    "    \"\"\"\n",
    "    Create a synthetic movie poster image for demonstration.\n",
    "    \"\"\"\n",
    "    # Create gradient background\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Create gradient from dark blue to black\n",
    "    for i in range(height):\n",
    "        ratio = i / height\n",
    "        img[i, :, 0] = int(20 * (1 - ratio))   # Blue\n",
    "        img[i, :, 1] = int(10 * (1 - ratio))   # Green\n",
    "        img[i, :, 2] = int(40 * (1 - ratio))   # Red (in BGR)\n",
    "    \n",
    "    # Add some shapes to represent movie poster elements\n",
    "    # Title area (rectangle)\n",
    "    cv2.rectangle(img, (50, 450), (350, 550), (200, 180, 100), -1)\n",
    "    \n",
    "    # Character silhouette (ellipse)\n",
    "    cv2.ellipse(img, (200, 250), (80, 150), 0, 0, 360, (100, 80, 60), -1)\n",
    "    \n",
    "    # Star rating\n",
    "    for i in range(5):\n",
    "        x = 80 + i * 60\n",
    "        cv2.circle(img, (x, 580), 15, (0, 200, 255), -1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create and display sample image\n",
    "sample_image = create_sample_poster()\n",
    "\n",
    "plt.figure(figsize=(6, 9))\n",
    "plt.imshow(cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Sample Movie Poster\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {sample_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Basic Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "# Define individual transforms\n",
    "transforms = {\n",
    "    'Original': None,\n",
    "    'Horizontal Flip': A.HorizontalFlip(p=1.0),\n",
    "    'Rotate 15': A.Rotate(limit=15, p=1.0),\n",
    "    'Brightness': A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0, p=1.0),\n",
    "    'Contrast': A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.3, p=1.0),\n",
    "    'Blur': A.GaussianBlur(blur_limit=(7, 7), p=1.0),\n",
    "}\n",
    "\n",
    "# Apply and display\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, transform) in enumerate(transforms.items()):\n",
    "    if transform is None:\n",
    "        augmented = sample_image\n",
    "    else:\n",
    "        augmented = transform(image=sample_image)['image']\n",
    "    \n",
    "    axes[idx].imshow(cv2.cvtColor(augmented, cv2.COLOR_BGR2RGB))\n",
    "    axes[idx].set_title(name, fontsize=12)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (Solved): Create an Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Create a composed augmentation pipeline\n",
    "\n",
    "poster_augmentation = A.Compose([\n",
    "    # Geometric transforms\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=5, p=0.5),\n",
    "    \n",
    "    # Color transforms\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1),\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # Noise and blur\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=3, p=1),\n",
    "        A.GaussNoise(var_limit=(10, 50), p=1),\n",
    "    ], p=0.3),\n",
    "])\n",
    "\n",
    "# Generate multiple augmentations\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    augmented = poster_augmentation(image=sample_image)['image']\n",
    "    axes[i].imshow(cv2.cvtColor(augmented, cv2.COLOR_BGR2RGB))\n",
    "    axes[i].set_title(f\"Augmented {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"10 Augmented Versions of the Same Poster\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Create a Domain-Specific Augmentation Pipeline\n",
    "\n",
    "Create an augmentation pipeline specifically for movie poster genre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an augmentation pipeline that:\n",
    "# 1. Preserves the overall composition (important for genre recognition)\n",
    "# 2. Simulates different viewing conditions (brightness, contrast)\n",
    "# 3. Is NOT too aggressive (we want to keep the poster recognizable)\n",
    "\n",
    "genre_classification_augmentation = A.Compose([\n",
    "    # Your augmentations here\n",
    "])\n",
    "\n",
    "# Test your pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Advanced Augmentations (Cutout, MixUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutout (CoarseDropout in Albumentations)\n",
    "cutout_transform = A.CoarseDropout(\n",
    "    max_holes=8,\n",
    "    max_height=50,\n",
    "    max_width=50,\n",
    "    min_holes=4,\n",
    "    min_height=20,\n",
    "    min_width=20,\n",
    "    fill_value=0,\n",
    "    p=1.0\n",
    ")\n",
    "\n",
    "# Apply and show\n",
    "cutout_image = cutout_transform(image=sample_image)['image']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "axes[0].imshow(cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(cv2.cvtColor(cutout_image, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(\"With Cutout\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: Implement MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MixUp augmentation\n",
    "# MixUp blends two images and their labels\n",
    "\n",
    "def mixup(image1, image2, label1, label2, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Perform MixUp augmentation.\n",
    "    \n",
    "    Args:\n",
    "        image1, image2: Two images to mix\n",
    "        label1, label2: Their corresponding labels (one-hot or probabilities)\n",
    "        alpha: Parameter for Beta distribution\n",
    "    \n",
    "    Returns:\n",
    "        mixed_image: Blended image\n",
    "        mixed_label: Blended label\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Create a second sample image\n",
    "sample_image2 = create_sample_poster()\n",
    "# Modify it slightly\n",
    "sample_image2 = cv2.rectangle(sample_image2.copy(), (100, 200), (300, 400), (0, 0, 255), -1)\n",
    "\n",
    "# Test your mixup function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5: Augmentation with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation that preserves bounding box annotations\n",
    "bbox_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Sample bounding boxes [x_min, y_min, x_max, y_max]\n",
    "bboxes = [\n",
    "    [50, 450, 350, 550],   # Title area\n",
    "    [120, 100, 280, 400],  # Character\n",
    "]\n",
    "labels = ['title', 'character']\n",
    "\n",
    "# Apply transform\n",
    "transformed = bbox_transform(\n",
    "    image=sample_image,\n",
    "    bboxes=bboxes,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "# Draw bounding boxes\n",
    "def draw_bboxes(image, bboxes, labels):\n",
    "    img = image.copy()\n",
    "    colors = {'title': (0, 255, 0), 'character': (255, 0, 0)}\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), colors.get(label, (0, 0, 255)), 2)\n",
    "        cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors.get(label, (0, 0, 255)), 2)\n",
    "    return img\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "orig_with_boxes = draw_bboxes(sample_image, bboxes, labels)\n",
    "axes[0].imshow(cv2.cvtColor(orig_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Original with Bboxes\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "aug_with_boxes = draw_bboxes(transformed['image'], transformed['bboxes'], transformed['labels'])\n",
    "axes[1].imshow(cv2.cvtColor(aug_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(\"Augmented with Bboxes\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Text Augmentation with nlpaug\n",
    "\n",
    "### 3.1 Setting Up nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "# Sample movie reviews\n",
    "sample_reviews = [\n",
    "    \"This movie was absolutely fantastic! A must-watch.\",\n",
    "    \"Terrible film. Waste of time and money.\",\n",
    "    \"The acting was superb, but the plot was confusing.\",\n",
    "    \"One of the best movies I have seen this year.\",\n",
    "    \"Boring and predictable. Would not recommend.\",\n",
    "]\n",
    "\n",
    "print(\"Sample reviews:\")\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    print(f\"{i+1}. {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): Synonym Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Synonym replacement augmentation\n",
    "\n",
    "# Create synonym augmenter using WordNet\n",
    "synonym_aug = naw.SynonymAug(aug_src='wordnet', aug_max=3)\n",
    "\n",
    "print(\"Synonym Augmentation Examples:\\n\")\n",
    "for review in sample_reviews[:3]:\n",
    "    augmented = synonym_aug.augment(review)\n",
    "    print(f\"Original: {review}\")\n",
    "    print(f\"Augmented: {augmented[0]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Random Word Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different word-level augmentations:\n",
    "# 1. Random word deletion\n",
    "# 2. Random word swap\n",
    "# 3. Random word insertion\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Contextual Word Embeddings (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual augmentation using BERT\n",
    "# Note: This requires downloading the BERT model (may take a few minutes)\n",
    "\n",
    "try:\n",
    "    bert_aug = naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-uncased',\n",
    "        action='substitute',\n",
    "        aug_max=2\n",
    "    )\n",
    "    \n",
    "    print(\"BERT Contextual Augmentation:\\n\")\n",
    "    for review in sample_reviews[:2]:\n",
    "        augmented = bert_aug.augment(review)\n",
    "        print(f\"Original: {review}\")\n",
    "        print(f\"Augmented: {augmented[0]}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"BERT augmentation not available: {e}\")\n",
    "    print(\"Try: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4: Character-Level Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level augmentation (simulating typos)\n",
    "\n",
    "# Keyboard typo augmenter\n",
    "keyboard_aug = nac.KeyboardAug(aug_char_max=2, aug_word_max=2)\n",
    "\n",
    "# OCR error augmenter\n",
    "ocr_aug = nac.OcrAug(aug_char_max=2, aug_word_max=2)\n",
    "\n",
    "print(\"Character-Level Augmentation Examples:\\n\")\n",
    "\n",
    "review = sample_reviews[0]\n",
    "print(f\"Original: {review}\")\n",
    "print(f\"Keyboard typos: {keyboard_aug.augment(review)[0]}\")\n",
    "print(f\"OCR errors: {ocr_aug.augment(review)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5: Create a Complete Text Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a pipeline that applies multiple augmentations\n",
    "# Use naf.Sequential or naf.Sometimes for probabilistic application\n",
    "\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "def create_text_augmentation_pipeline():\n",
    "    \"\"\"\n",
    "    Create a text augmentation pipeline for movie review sentiment.\n",
    "    \n",
    "    Requirements:\n",
    "    - Should preserve sentiment (don't replace sentiment words)\n",
    "    - Should maintain readability\n",
    "    - Should be diverse\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.6: Back-Translation (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-translation: English -> Other Language -> English\n",
    "# This requires downloading translation models\n",
    "\n",
    "try:\n",
    "    back_trans_aug = naw.BackTranslationAug(\n",
    "        from_model_name='facebook/wmt19-en-de',\n",
    "        to_model_name='facebook/wmt19-de-en'\n",
    "    )\n",
    "    \n",
    "    print(\"Back-Translation Augmentation:\\n\")\n",
    "    review = \"This movie was absolutely fantastic!\"\n",
    "    augmented = back_trans_aug.augment(review)\n",
    "    print(f\"Original: {review}\")\n",
    "    print(f\"Back-translated: {augmented[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Back-translation not available: {e}\")\n",
    "    print(\"Requires: pip install transformers torch sentencepiece\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Audio Augmentation\n",
    "\n",
    "### 4.1 Creating Sample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a synthetic audio signal (sine wave + noise)\n",
    "def create_sample_audio(duration=2.0, sr=16000):\n",
    "    \"\"\"\n",
    "    Create a sample audio signal.\n",
    "    \n",
    "    Args:\n",
    "        duration: Duration in seconds\n",
    "        sr: Sample rate\n",
    "    \n",
    "    Returns:\n",
    "        audio: Audio signal as numpy array\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
    "    \n",
    "    # Create a combination of frequencies (like a chord)\n",
    "    audio = (\n",
    "        0.3 * np.sin(2 * np.pi * 440 * t) +   # A4 note\n",
    "        0.2 * np.sin(2 * np.pi * 554 * t) +   # C#5 note\n",
    "        0.2 * np.sin(2 * np.pi * 659 * t) +   # E5 note\n",
    "        0.1 * np.random.randn(len(t))          # Some noise\n",
    "    )\n",
    "    \n",
    "    # Normalize\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "sample_audio = create_sample_audio()\n",
    "sr = 16000\n",
    "\n",
    "print(f\"Audio shape: {sample_audio.shape}\")\n",
    "print(f\"Duration: {len(sample_audio) / sr:.2f} seconds\")\n",
    "\n",
    "# Plot waveform\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(np.linspace(0, len(sample_audio)/sr, len(sample_audio)), sample_audio)\n",
    "plt.title(\"Sample Audio Waveform\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Basic Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual audio augmentations\n",
    "\n",
    "def add_noise(audio, noise_level=0.01):\n",
    "    \"\"\"Add Gaussian noise to audio.\"\"\"\n",
    "    noise = np.random.randn(len(audio)) * noise_level\n",
    "    return audio + noise\n",
    "\n",
    "def time_shift(audio, shift_range=0.2):\n",
    "    \"\"\"Shift audio in time.\"\"\"\n",
    "    shift = int(len(audio) * shift_range * np.random.uniform(-1, 1))\n",
    "    return np.roll(audio, shift)\n",
    "\n",
    "def change_volume(audio, volume_range=(0.5, 1.5)):\n",
    "    \"\"\"Change audio volume.\"\"\"\n",
    "    volume = np.random.uniform(*volume_range)\n",
    "    return audio * volume\n",
    "\n",
    "def pitch_shift_simple(audio, shift_factor=0.1):\n",
    "    \"\"\"Simple pitch shift by resampling.\"\"\"\n",
    "    from scipy.signal import resample\n",
    "    factor = 1 + np.random.uniform(-shift_factor, shift_factor)\n",
    "    new_length = int(len(audio) / factor)\n",
    "    shifted = resample(audio, new_length)\n",
    "    # Pad or trim to original length\n",
    "    if len(shifted) > len(audio):\n",
    "        return shifted[:len(audio)]\n",
    "    else:\n",
    "        return np.pad(shifted, (0, len(audio) - len(shifted)))\n",
    "\n",
    "# Apply augmentations\n",
    "augmentations = {\n",
    "    'Original': sample_audio,\n",
    "    'Added Noise': add_noise(sample_audio, 0.05),\n",
    "    'Time Shifted': time_shift(sample_audio, 0.1),\n",
    "    'Volume Changed': change_volume(sample_audio, (0.5, 1.5)),\n",
    "}\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(12, 8))\n",
    "for idx, (name, audio) in enumerate(augmentations.items()):\n",
    "    axes[idx].plot(audio)\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].set_xlim(0, len(audio))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Implement SpecAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SpecAugment (frequency and time masking on spectrograms)\n",
    "\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "def spec_augment(audio, sr, freq_mask_param=10, time_mask_param=20):\n",
    "    \"\"\"\n",
    "    Apply SpecAugment to audio.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio signal\n",
    "        sr: Sample rate\n",
    "        freq_mask_param: Maximum frequency mask width\n",
    "        time_mask_param: Maximum time mask width\n",
    "    \n",
    "    Returns:\n",
    "        Augmented spectrogram\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Measuring Augmentation Impact\n",
    "\n",
    "### 5.1 Setting Up a Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a small subset of 20 newsgroups\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, \n",
    "                                 remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Limit to smaller dataset for demonstration\n",
    "texts = newsgroups.data[:500]\n",
    "labels = newsgroups.target[:500]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (Solved): Baseline Without Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Train baseline model without augmentation\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test_vec)\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Baseline Accuracy (no augmentation): {baseline_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Train with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Augment training data and measure improvement\n",
    "# 1. Augment each training sample 2-3 times\n",
    "# 2. Combine with original data\n",
    "# 3. Train model\n",
    "# 4. Compare accuracy\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3: Learning Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot learning curves with and without augmentation\n",
    "# Train on 10%, 20%, ..., 100% of training data\n",
    "# Compare the two curves\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Best Practices and Guidelines\n",
    "\n",
    "### Question 6.1: When NOT to Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Bad augmentation that changes the label\n",
    "\n",
    "# Example 1: Text sentiment - replacing key sentiment words\n",
    "review = \"This movie was terrible\"\n",
    "# Bad augmentation: synonym replacement of \"terrible\"\n",
    "bad_aug = \"This movie was awesome\"  # Completely changes meaning!\n",
    "\n",
    "print(\"Example of BAD text augmentation:\")\n",
    "print(f\"Original (NEGATIVE): {review}\")\n",
    "print(f\"Bad augmentation: {bad_aug}\")\n",
    "print(\"The label changed from NEGATIVE to POSITIVE!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Digit classification with vertical flip\n",
    "print(\"Example of BAD image augmentation:\")\n",
    "print(\"Original: Image of '6'\")\n",
    "print(\"Vertical flip: Image of '9'\")\n",
    "print(\"The label changed from '6' to '9'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2: Create an Augmentation Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a checklist for designing augmentation pipelines\n",
    "\n",
    "augmentation_checklist = \"\"\"\n",
    "# Augmentation Design Checklist\n",
    "\n",
    "## Before Choosing Augmentations:\n",
    "[ ] What is the task? (classification, detection, segmentation, etc.)\n",
    "[ ] What modality? (image, text, audio, video)\n",
    "[ ] What domain? (natural images, medical, documents, etc.)\n",
    "\n",
    "## Validation Questions:\n",
    "[ ] Does this augmentation preserve the label?\n",
    "[ ] ...\n",
    "[ ] ...\n",
    "\n",
    "## Implementation:\n",
    "[ ] ...\n",
    "\n",
    "## Testing:\n",
    "[ ] ...\n",
    "\"\"\"\n",
    "\n",
    "# Complete the checklist\n",
    "print(augmentation_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems\n",
    "\n",
    "### Challenge 1: Test-Time Augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement Test-Time Augmentation\n",
    "# Apply augmentations at inference time and average predictions\n",
    "\n",
    "def tta_predict(model, vectorizer, text, n_augments=5):\n",
    "    \"\"\"\n",
    "    Make prediction using Test-Time Augmentation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        vectorizer: Fitted TfidfVectorizer\n",
    "        text: Text to classify\n",
    "        n_augments: Number of augmented versions to use\n",
    "    \n",
    "    Returns:\n",
    "        Averaged prediction probabilities\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: AutoAugment-Style Policy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement a simple augmentation policy search\n",
    "# Find the best combination of augmentation parameters\n",
    "\n",
    "def search_augmentation_policy(X_train, y_train, X_val, y_val, n_trials=20):\n",
    "    \"\"\"\n",
    "    Search for the best augmentation policy using random search.\n",
    "    \n",
    "    Returns:\n",
    "        best_policy: Dictionary of best augmentation parameters\n",
    "        best_score: Validation accuracy with best policy\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Augmentation for Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Use augmentation to balance an imbalanced dataset\n",
    "# More augmentation for minority classes\n",
    "\n",
    "def class_balanced_augmentation(X, y, target_count=None):\n",
    "    \"\"\"\n",
    "    Augment data to balance class distribution.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        target_count: Target number of samples per class (default: max class count)\n",
    "    \n",
    "    Returns:\n",
    "        X_balanced, y_balanced: Balanced dataset\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Image Augmentation**: Geometric and color transforms with Albumentations\n",
    "2. **Text Augmentation**: Synonym replacement, back-translation with nlpaug\n",
    "3. **Audio Augmentation**: Time-domain and frequency-domain augmentations\n",
    "4. **Pipeline Design**: Creating appropriate augmentation pipelines for tasks\n",
    "5. **Impact Measurement**: Comparing model performance with/without augmentation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Modality | Key Transforms | Libraries |\n",
    "|----------|---------------|----------|\n",
    "| Image | Flip, rotate, color, cutout | Albumentations |\n",
    "| Text | Synonym, back-translation | nlpaug |\n",
    "| Audio | Noise, pitch, SpecAugment | audiomentations |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always verify** augmentations preserve labels\n",
    "2. **Start simple**, add complexity gradually\n",
    "3. **Don't augment** validation/test data\n",
    "4. **Measure impact** before and after\n",
    "5. **Domain-specific** choices matter\n",
    "\n",
    "### Next Week\n",
    "\n",
    "Week 6: LLM APIs - Integrating large language models into your applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
