{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Lab: LLM APIs & Prompt Engineering\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**  \n",
    "**IIT Gandhinagar**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Set up and use **Gemini API** (free tier) and **OpenRouter** (free models)\n",
    "2. Apply prompt engineering techniques (zero-shot, few-shot, chain-of-thought)\n",
    "3. Use LLMs for **data labeling** (connecting to Weeks 3-4)\n",
    "4. Use LLMs for **text augmentation** (connecting to Week 5)\n",
    "5. Extract structured data using JSON mode\n",
    "6. Build practical NLP pipelines with LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## Connection to Previous Weeks\n",
    "\n",
    "| Previous Week | What We Did | How LLMs Help Today |\n",
    "|---------------|-------------|--------------------|\n",
    "| Week 1: Data Collection | Collected movie data via APIs | Parse unstructured text to JSON |\n",
    "| Week 2: Data Validation | Validated with Pydantic schemas | Fix/normalize messy data |\n",
    "| Week 3: Data Labeling | Manual annotation, Cohen's Kappa | **Auto-label 100x faster** |\n",
    "| Week 4: Optimizing Labeling | Active learning, weak supervision | **LLM as labeling function** |\n",
    "| Week 5: Data Augmentation | nlpaug, Albumentations | **Generate paraphrases** |\n",
    "\n",
    "**Today's Goal**: Use LLMs to supercharge our Netflix movie pipeline!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "### 1.1 Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-genai openai requests pandas numpy matplotlib seaborn pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 API Key Setup\n",
    "\n",
    "You have **two free options**:\n",
    "\n",
    "1. **Gemini API** (Recommended): [aistudio.google.com/apikey](https://aistudio.google.com/apikey)\n",
    "2. **OpenRouter**: [openrouter.ai/keys](https://openrouter.ai/keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set API keys directly (for quick testing)\n",
    "# WARNING: Don't commit these to git!\n",
    "\n",
    "# Uncomment and fill in your keys:\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-gemini-key-here'\n",
    "# os.environ['OPENROUTER_API_KEY'] = 'your-openrouter-key-here'\n",
    "\n",
    "# Option 2: Load from environment (recommended for production)\n",
    "GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY', '')\n",
    "OPENROUTER_API_KEY = os.environ.get('OPENROUTER_API_KEY', '')\n",
    "\n",
    "print(f\"Gemini API Key configured: {'Yes' if GEMINI_API_KEY else 'No'}\")\n",
    "print(f\"OpenRouter API Key configured: {'Yes' if OPENROUTER_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setting Up LLM Clients\n",
    "\n",
    "### 2.1 Gemini Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API Setup\n",
    "try:\n",
    "    from google import genai\n",
    "    \n",
    "    if GEMINI_API_KEY:\n",
    "        gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        GEMINI_MODEL = \"gemini-2.0-flash-exp\"  # Fast and free\n",
    "        print(f\"Gemini client initialized with model: {GEMINI_MODEL}\")\n",
    "    else:\n",
    "        gemini_client = None\n",
    "        print(\"Gemini client not initialized (no API key)\")\n",
    "        \n",
    "except ImportError:\n",
    "    gemini_client = None\n",
    "    print(\"google-genai not installed. Run: pip install google-genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 OpenRouter Client Setup\n",
    "\n",
    "OpenRouter provides access to **100+ models** with a unified API. Many are **free**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter Setup - uses OpenAI-compatible API\n",
    "import openai\n",
    "\n",
    "if OPENROUTER_API_KEY:\n",
    "    openrouter_client = openai.OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY\n",
    "    )\n",
    "    print(\"OpenRouter client initialized!\")\n",
    "else:\n",
    "    openrouter_client = None\n",
    "    print(\"OpenRouter client not initialized (no API key)\")\n",
    "\n",
    "# Free models on OpenRouter (as of 2024-2025)\n",
    "FREE_MODELS = {\n",
    "    \"llama-3.1-8b\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
    "    \"gemma-2-9b\": \"google/gemma-2-9b-it:free\",\n",
    "    \"phi-3-mini\": \"microsoft/phi-3-mini-128k-instruct:free\",\n",
    "    \"mistral-7b\": \"mistralai/mistral-7b-instruct:free\",\n",
    "    \"qwen-2-7b\": \"qwen/qwen-2-7b-instruct:free\",\n",
    "}\n",
    "\n",
    "print(\"\\nFree models available:\")\n",
    "for name, model_id in FREE_MODELS.items():\n",
    "    print(f\"  - {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Unified LLM Interface\n",
    "\n",
    "Let's create a unified interface that works with both providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, provider=\"gemini\", model=None, temperature=0.7, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    Unified text generation interface for Gemini and OpenRouter.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt\n",
    "        provider: \"gemini\" or \"openrouter\"\n",
    "        model: Model name (uses defaults if None)\n",
    "        temperature: Creativity (0=deterministic, 1=creative)\n",
    "        max_tokens: Maximum output length\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    if provider == \"gemini\" and gemini_client:\n",
    "        model = model or GEMINI_MODEL\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=prompt,\n",
    "            config={\"temperature\": temperature, \"max_output_tokens\": max_tokens}\n",
    "        )\n",
    "        return response.text\n",
    "    \n",
    "    elif provider == \"openrouter\" and openrouter_client:\n",
    "        model = model or FREE_MODELS[\"llama-3.1-8b\"]\n",
    "        response = openrouter_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        return f\"[Mock response] No {provider} client available. Prompt was: {prompt[:100]}...\"\n",
    "\n",
    "# Test both providers\n",
    "test_prompt = \"What is 2 + 2? Answer in one word.\"\n",
    "\n",
    "print(\"Testing Gemini:\")\n",
    "print(generate_text(test_prompt, provider=\"gemini\"))\n",
    "\n",
    "print(\"\\nTesting OpenRouter (Llama 3.1):\")\n",
    "print(generate_text(test_prompt, provider=\"openrouter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Our Netflix Movie Dataset\n",
    "\n",
    "Let's continue with our movie theme from previous weeks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie reviews from our Netflix pipeline\n",
    "movie_reviews = [\n",
    "    {\"id\": 1, \"movie\": \"Inception\", \"review\": \"Mind-blowing! Nolan does it again with this masterpiece. The layers of dreams within dreams kept me on the edge of my seat.\"},\n",
    "    {\"id\": 2, \"movie\": \"The Room\", \"review\": \"So bad it's good. Hilarious unintentionally. Tommy Wiseau's acting is legendarily terrible.\"},\n",
    "    {\"id\": 3, \"movie\": \"Parasite\", \"review\": \"Gripping from start to finish. Deserved every Oscar. Bong Joon-ho is a genius.\"},\n",
    "    {\"id\": 4, \"movie\": \"Cats\", \"review\": \"What did I just watch? Truly bizarre and unsettling. Those CGI cats will haunt my nightmares.\"},\n",
    "    {\"id\": 5, \"movie\": \"The Godfather\", \"review\": \"A timeless classic. Marlon Brando's performance is perfect. The cinematography is stunning.\"},\n",
    "    {\"id\": 6, \"movie\": \"Avatar\", \"review\": \"Visually stunning but the story is predictable. James Cameron knows how to make a spectacle.\"},\n",
    "    {\"id\": 7, \"movie\": \"The Dark Knight\", \"review\": \"Heath Ledger's Joker is unforgettable. Best superhero movie ever made.\"},\n",
    "    {\"id\": 8, \"movie\": \"Twilight\", \"review\": \"Not my cup of tea but I can see the appeal for the target audience.\"},\n",
    "    {\"id\": 9, \"movie\": \"Interstellar\", \"review\": \"Made me cry. Beautiful exploration of love and time. Hans Zimmer's score is incredible.\"},\n",
    "    {\"id\": 10, \"movie\": \"Emoji Movie\", \"review\": \"Just... no. Avoid at all costs. A soulless cash grab.\"},\n",
    "    {\"id\": 11, \"movie\": \"Pulp Fiction\", \"review\": \"Tarantino's dialogue is unmatched. Non-linear storytelling at its finest.\"},\n",
    "    {\"id\": 12, \"movie\": \"Sharknado\", \"review\": \"Ridiculous premise but entertaining in a weird way. Perfect for a bad movie night.\"},\n",
    "    {\"id\": 13, \"movie\": \"The Shawshank Redemption\", \"review\": \"Hope is a good thing. Best movie ever made in my opinion. Tim Robbins and Morgan Freeman are incredible.\"},\n",
    "    {\"id\": 14, \"movie\": \"Transformers 5\", \"review\": \"Explosions. That's it. That's the review. Michael Bay gonna Michael Bay.\"},\n",
    "    {\"id\": 15, \"movie\": \"La La Land\", \"review\": \"Bittersweet ending that stays with you. Ryan Gosling and Emma Stone have great chemistry.\"},\n",
    "]\n",
    "\n",
    "df_reviews = pd.DataFrame(movie_reviews)\n",
    "print(f\"Loaded {len(df_reviews)} movie reviews\")\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Sentiment Classification (Zero-Shot)\n",
    "\n",
    "### 4.1 Basic Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Zero-shot sentiment classification\n",
    "\n",
    "def classify_sentiment_zero_shot(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Classify sentiment using zero-shot prompting.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Classify the sentiment of this movie review as exactly one of: Positive, Negative, or Neutral.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Respond with only the sentiment label (Positive, Negative, or Neutral).\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0)\n",
    "    return response.strip()\n",
    "\n",
    "# Test on a few reviews\n",
    "print(\"Zero-Shot Sentiment Classification:\\n\")\n",
    "for _, row in df_reviews.head(5).iterrows():\n",
    "    sentiment = classify_sentiment_zero_shot(row['review'])\n",
    "    print(f\"Movie: {row['movie']}\")\n",
    "    print(f\"Review: {row['review'][:80]}...\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Batch Classification\n",
    "\n",
    "Classify all reviews and add the sentiment to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Classify all reviews and add a 'sentiment' column to df_reviews\n",
    "# Use a loop with rate limiting (time.sleep(1) between requests)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Few-Shot Learning\n",
    "\n",
    "### 5.1 (Solved) Few-Shot Sentiment with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Few-shot classification with examples\n",
    "\n",
    "def classify_sentiment_few_shot(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Classify sentiment using few-shot prompting with examples.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"Classify movie reviews as Positive, Negative, or Mixed.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Review: \"Amazing film! Best I've seen this year. A must-watch!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"Terrible waste of time. The acting was wooden and the plot made no sense.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"Visually stunning but the story is weak. Great effects, poor writing.\"\n",
    "Sentiment: Mixed\n",
    "\n",
    "Review: \"It was okay. Nothing special but not terrible either.\"\n",
    "Sentiment: Mixed\n",
    "\n",
    "Now classify this review:\n",
    "\n",
    "Review: \"{review}\"\n",
    "Sentiment:\"\"\".format(review=review)\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0)\n",
    "    return response.strip()\n",
    "\n",
    "# Test\n",
    "test_reviews = [\n",
    "    \"Visually stunning but the story is predictable.\",\n",
    "    \"So bad it's good. Hilarious unintentionally.\",\n",
    "    \"Perfect in every way. A masterpiece.\"\n",
    "]\n",
    "\n",
    "print(\"Few-Shot Classification:\\n\")\n",
    "for review in test_reviews:\n",
    "    sentiment = classify_sentiment_few_shot(review)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1: Create Your Own Few-Shot Classifier\n",
    "\n",
    "Create a few-shot classifier for **movie genre** based on the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a few-shot genre classifier\n",
    "# Genres: Action, Comedy, Drama, Horror, Sci-Fi, Romance\n",
    "# Provide 2-3 examples for each genre\n",
    "\n",
    "def classify_genre_few_shot(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Classify movie genre from review using few-shot prompting.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"Classify the likely genre of this movie based on the review.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# Add your examples here\n",
    "\n",
    "Now classify:\n",
    "\n",
    "Review: \"{review}\"\n",
    "Genre:\"\"\".format(review=review)\n",
    "    \n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: LLM-Based Data Labeling (Week 3-4 Connection)\n",
    "\n",
    "Remember Week 3-4? We spent effort on manual labeling and active learning. LLMs can **accelerate labeling 10-100x**!\n",
    "\n",
    "### 6.1 (Solved) Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Multi-label classification for movie review attributes\n",
    "\n",
    "def label_review_attributes(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Label multiple attributes of a movie review.\n",
    "    Returns structured JSON with multiple labels.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Analyze this movie review and provide labels for the following attributes:\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Provide your analysis in this exact JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"Positive\" or \"Negative\" or \"Mixed\",\n",
    "    \"mentions_acting\": true or false,\n",
    "    \"mentions_visuals\": true or false,\n",
    "    \"mentions_story\": true or false,\n",
    "    \"mentions_director\": true or false,\n",
    "    \"would_recommend\": true or false,\n",
    "    \"intensity\": \"Strong\" or \"Moderate\" or \"Mild\"\n",
    "}}\n",
    "\n",
    "Respond with ONLY the JSON, no other text.\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0)\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    try:\n",
    "        # Clean up response (remove markdown code blocks if present)\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.split(\"```\")[1]\n",
    "            if cleaned.startswith(\"json\"):\n",
    "                cleaned = cleaned[4:]\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\", \"raw\": response}\n",
    "\n",
    "# Test on a review\n",
    "test_review = \"Heath Ledger's Joker is unforgettable. Best superhero movie ever made.\"\n",
    "labels = label_review_attributes(test_review)\n",
    "\n",
    "print(f\"Review: {test_review}\")\n",
    "print(f\"\\nLabels:\")\n",
    "print(json.dumps(labels, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 (Solved) Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Extract named entities from reviews\n",
    "\n",
    "def extract_entities(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Extract named entities (people, movies, etc.) from review.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Extract all named entities from this movie review.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Categories:\n",
    "- PERSON: Actors, directors, characters\n",
    "- MOVIE: Movie titles mentioned\n",
    "- AWARD: Awards or accolades\n",
    "- ORGANIZATION: Studios, production companies\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"PERSON\": [\"name1\", \"name2\"],\n",
    "    \"MOVIE\": [\"movie1\"],\n",
    "    \"AWARD\": [\"award1\"],\n",
    "    \"ORGANIZATION\": [\"org1\"]\n",
    "}}\n",
    "\n",
    "Only include categories with entities found. Respond with ONLY JSON.\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0)\n",
    "    \n",
    "    try:\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.split(\"```\")[1]\n",
    "            if cleaned.startswith(\"json\"):\n",
    "                cleaned = cleaned[4:]\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse\", \"raw\": response}\n",
    "\n",
    "# Test\n",
    "test_reviews = [\n",
    "    \"Heath Ledger's Joker in The Dark Knight is unforgettable. Christopher Nolan is a genius.\",\n",
    "    \"Bong Joon-ho's Parasite deserved every Oscar it won. The Oscars got it right for once.\",\n",
    "]\n",
    "\n",
    "print(\"Named Entity Recognition:\\n\")\n",
    "for review in test_reviews:\n",
    "    entities = extract_entities(review)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Entities: {json.dumps(entities, indent=2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1: LLM as Labeling Function (Snorkel Connection)\n",
    "\n",
    "In Week 4, we used Snorkel labeling functions. Create an LLM-based labeling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an LLM-based labeling function for Snorkel\n",
    "# The function should:\n",
    "# 1. Take a review text\n",
    "# 2. Return 1 (Positive), 0 (Negative), or -1 (Abstain)\n",
    "# 3. Only label high-confidence cases\n",
    "\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "def lf_llm_sentiment(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    LLM-based labeling function for sentiment.\n",
    "    Returns ABSTAIN for uncertain cases.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Rate your confidence in the sentiment of this review.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"Positive\" or \"Negative\" or \"Neutral\",\n",
    "    \"confidence\": 0.0 to 1.0\n",
    "}}\n",
    "\n",
    "Only JSON, no other text.\"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    # Parse response and return POSITIVE, NEGATIVE, or ABSTAIN based on confidence\n",
    "    pass\n",
    "\n",
    "# Test your labeling function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2: Batch Labeling with Cost Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a batch labeling function that:\n",
    "# 1. Labels multiple reviews\n",
    "# 2. Tracks the number of API calls\n",
    "# 3. Estimates cost (assume $0.001 per 1K tokens)\n",
    "# 4. Handles rate limiting with sleep\n",
    "\n",
    "def batch_label_reviews(reviews, provider=\"gemini\", delay=1.0):\n",
    "    \"\"\"\n",
    "    Label multiple reviews with cost tracking.\n",
    "    \n",
    "    Returns:\n",
    "        labels: List of sentiment labels\n",
    "        stats: Dict with api_calls, estimated_cost, etc.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test on first 5 reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: LLM-Based Data Augmentation (Week 5 Connection)\n",
    "\n",
    "Remember Week 5? We used nlpaug for text augmentation. LLMs can generate **more natural paraphrases**!\n",
    "\n",
    "### 7.1 (Solved) Paraphrase Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Generate paraphrases using LLM\n",
    "\n",
    "def generate_paraphrases(text, n=3, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Generate n paraphrases of the input text.\n",
    "    Maintains the original meaning and sentiment.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate {n} different paraphrases of this movie review.\n",
    "Keep the same sentiment and meaning, but vary the wording.\n",
    "\n",
    "Original: \"{text}\"\n",
    "\n",
    "Respond with a JSON array of {n} paraphrases:\n",
    "[\"paraphrase 1\", \"paraphrase 2\", ...]\n",
    "\n",
    "Only JSON, no other text.\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0.7)\n",
    "    \n",
    "    try:\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.split(\"```\")[1]\n",
    "            if cleaned.startswith(\"json\"):\n",
    "                cleaned = cleaned[4:]\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        return [response]  # Return raw response if parsing fails\n",
    "\n",
    "# Test\n",
    "original = \"Mind-blowing! Nolan does it again with this masterpiece.\"\n",
    "paraphrases = generate_paraphrases(original, n=3)\n",
    "\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"\\nParaphrases:\")\n",
    "for i, p in enumerate(paraphrases, 1):\n",
    "    print(f\"  {i}. {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 (Solved) Style Transfer for Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Style transfer - rewrite in different styles\n",
    "\n",
    "def style_transfer(text, style, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Rewrite text in a different style while keeping sentiment.\n",
    "    \n",
    "    Styles: formal, casual, enthusiastic, critical, brief, detailed\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Rewrite this movie review in a {style} style.\n",
    "Keep the same sentiment (positive/negative) but change the writing style.\n",
    "\n",
    "Original: \"{text}\"\n",
    "\n",
    "Rewritten ({style} style):\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0.7)\n",
    "    return response.strip()\n",
    "\n",
    "# Test different styles\n",
    "original = \"This movie was absolutely fantastic! A must-watch.\"\n",
    "styles = [\"formal\", \"casual\", \"enthusiastic\", \"brief\"]\n",
    "\n",
    "print(f\"Original: {original}\\n\")\n",
    "for style in styles:\n",
    "    rewritten = style_transfer(original, style)\n",
    "    print(f\"{style.capitalize()}: {rewritten}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7.1: Augment Training Data\n",
    "\n",
    "Create an augmentation pipeline that expands our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function that augments a dataset by:\n",
    "# 1. Generating 2 paraphrases per review\n",
    "# 2. Applying one style transfer per review\n",
    "# 3. Keeping track of original vs augmented samples\n",
    "\n",
    "def augment_dataset(reviews, labels, augmentation_factor=3, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Augment a dataset of reviews.\n",
    "    \n",
    "    Args:\n",
    "        reviews: List of review texts\n",
    "        labels: List of sentiment labels\n",
    "        augmentation_factor: How many augmented samples per original\n",
    "    \n",
    "    Returns:\n",
    "        augmented_reviews: List including originals + augmented\n",
    "        augmented_labels: Corresponding labels\n",
    "        is_augmented: Boolean list (True if augmented)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test on a small sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Chain-of-Thought Reasoning\n",
    "\n",
    "### 8.1 (Solved) CoT for Complex Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Chain-of-thought for nuanced review analysis\n",
    "\n",
    "def analyze_review_cot(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Analyze a review step-by-step using chain-of-thought.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Analyze this movie review step by step.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Think through this carefully:\n",
    "\n",
    "Step 1: What specific aspects of the movie does the reviewer mention?\n",
    "Step 2: For each aspect, is the reviewer positive, negative, or neutral?\n",
    "Step 3: What is the overall tone of the review?\n",
    "Step 4: Would the reviewer recommend this movie?\n",
    "Step 5: Final sentiment classification and confidence.\n",
    "\n",
    "Provide your analysis:\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0.3)\n",
    "    return response\n",
    "\n",
    "# Test on a nuanced review\n",
    "nuanced_review = \"Visually stunning but the story is predictable. James Cameron knows how to make a spectacle.\"\n",
    "analysis = analyze_review_cot(nuanced_review)\n",
    "\n",
    "print(f\"Review: {nuanced_review}\")\n",
    "print(f\"\\nChain-of-Thought Analysis:\\n{analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.1: CoT for Comparison\n",
    "\n",
    "Use chain-of-thought to compare two reviews and determine which movie is better reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a CoT function that compares two reviews\n",
    "\n",
    "def compare_reviews_cot(review1, movie1, review2, movie2, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Compare two movie reviews using chain-of-thought.\n",
    "    \n",
    "    Returns:\n",
    "        winner: Which movie is better reviewed\n",
    "        reasoning: The step-by-step analysis\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test: Compare Inception vs Avatar reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Structured Output with Pydantic\n",
    "\n",
    "### 9.1 (Solved) Pydantic Models for Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Structured output with Pydantic models\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class ReviewAnalysis(BaseModel):\n",
    "    \"\"\"Structured analysis of a movie review.\"\"\"\n",
    "    sentiment: str = Field(description=\"Overall sentiment: Positive, Negative, or Mixed\")\n",
    "    confidence: float = Field(ge=0, le=1, description=\"Confidence score 0-1\")\n",
    "    key_points: List[str] = Field(description=\"Main points from the review\")\n",
    "    mentioned_aspects: List[str] = Field(description=\"Aspects mentioned: acting, visuals, story, etc.\")\n",
    "    recommendation: bool = Field(description=\"Would the reviewer recommend this movie?\")\n",
    "    summary: str = Field(description=\"One-sentence summary of the review\")\n",
    "\n",
    "def analyze_review_structured(review, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Analyze review and return structured Pydantic model.\n",
    "    \"\"\"\n",
    "    schema = ReviewAnalysis.model_json_schema()\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this movie review and provide structured output.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Respond with JSON matching this schema:\n",
    "{json.dumps(schema['properties'], indent=2)}\n",
    "\n",
    "Only JSON, no other text.\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0)\n",
    "    \n",
    "    try:\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.split(\"```\")[1]\n",
    "            if cleaned.startswith(\"json\"):\n",
    "                cleaned = cleaned[4:]\n",
    "        data = json.loads(cleaned)\n",
    "        return ReviewAnalysis(**data)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"raw\": response}\n",
    "\n",
    "# Test\n",
    "review = \"Heath Ledger's Joker is unforgettable. Best superhero movie ever made.\"\n",
    "analysis = analyze_review_structured(review)\n",
    "\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"\\nStructured Analysis:\")\n",
    "if isinstance(analysis, ReviewAnalysis):\n",
    "    print(f\"  Sentiment: {analysis.sentiment} (confidence: {analysis.confidence})\")\n",
    "    print(f\"  Key Points: {analysis.key_points}\")\n",
    "    print(f\"  Aspects: {analysis.mentioned_aspects}\")\n",
    "    print(f\"  Recommend: {analysis.recommendation}\")\n",
    "    print(f\"  Summary: {analysis.summary}\")\n",
    "else:\n",
    "    print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9.1: Create Custom Pydantic Model\n",
    "\n",
    "Create a Pydantic model for extracting movie metadata from reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a MovieMetadata Pydantic model that extracts:\n",
    "# - likely_genre (list of genres)\n",
    "# - mentioned_actors (list of names)\n",
    "# - mentioned_director (optional string)\n",
    "# - year_hints (optional int if mentioned)\n",
    "# - similar_movies_mentioned (list of movie titles)\n",
    "\n",
    "class MovieMetadata(BaseModel):\n",
    "    # Your fields here\n",
    "    pass\n",
    "\n",
    "def extract_movie_metadata(review, provider=\"gemini\"):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test on a review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Comparing Models\n",
    "\n",
    "### 10.1 (Solved) Model Comparison on Same Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Compare different models on the same task\n",
    "\n",
    "def compare_models(review, task=\"sentiment\"):\n",
    "    \"\"\"\n",
    "    Run the same prompt on multiple models and compare results.\n",
    "    \"\"\"\n",
    "    if task == \"sentiment\":\n",
    "        prompt = f\"\"\"Classify this review as Positive, Negative, or Mixed.\n",
    "Review: \"{review}\"\n",
    "Sentiment:\"\"\"\n",
    "    else:\n",
    "        prompt = task  # Use as custom prompt\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Gemini\n",
    "    if gemini_client:\n",
    "        try:\n",
    "            results[\"Gemini Flash\"] = generate_text(prompt, provider=\"gemini\")\n",
    "        except Exception as e:\n",
    "            results[\"Gemini Flash\"] = f\"Error: {e}\"\n",
    "    \n",
    "    # OpenRouter models\n",
    "    if openrouter_client:\n",
    "        for name, model_id in list(FREE_MODELS.items())[:3]:  # Test first 3 free models\n",
    "            try:\n",
    "                results[name] = generate_text(prompt, provider=\"openrouter\", model=model_id)\n",
    "                time.sleep(1)  # Rate limiting\n",
    "            except Exception as e:\n",
    "                results[name] = f\"Error: {e}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare on a tricky review\n",
    "tricky_review = \"So bad it's good. Hilarious unintentionally.\"\n",
    "results = compare_models(tricky_review)\n",
    "\n",
    "print(f\"Review: {tricky_review}\")\n",
    "print(f\"\\nModel Responses:\")\n",
    "for model, response in results.items():\n",
    "    print(f\"  {model}: {response.strip()[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10.1: Benchmark Models\n",
    "\n",
    "Create a benchmark comparing model accuracy on labeled test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a benchmark with ground truth labels\n",
    "\n",
    "# Ground truth labels for our reviews\n",
    "ground_truth = {\n",
    "    1: \"Positive\",   # Inception\n",
    "    2: \"Mixed\",      # The Room (so bad it's good)\n",
    "    3: \"Positive\",   # Parasite\n",
    "    4: \"Negative\",   # Cats\n",
    "    5: \"Positive\",   # The Godfather\n",
    "    6: \"Mixed\",      # Avatar (stunning but predictable)\n",
    "    7: \"Positive\",   # The Dark Knight\n",
    "    8: \"Neutral\",    # Twilight\n",
    "    9: \"Positive\",   # Interstellar\n",
    "    10: \"Negative\",  # Emoji Movie\n",
    "}\n",
    "\n",
    "def benchmark_models(reviews_df, ground_truth, providers=[\"gemini\"]):\n",
    "    \"\"\"\n",
    "    Benchmark models on labeled data.\n",
    "    \n",
    "    Returns:\n",
    "        results: Dict with accuracy per model\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Run benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Cost-Effective Strategies\n",
    "\n",
    "### 11.1 (Solved) Batching Multiple Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: Batch multiple reviews in one API call\n",
    "\n",
    "def batch_classify(reviews, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Classify multiple reviews in a single API call.\n",
    "    More cost-effective than individual calls.\n",
    "    \"\"\"\n",
    "    # Format reviews with numbers\n",
    "    formatted = \"\\n\".join([f\"{i+1}. \\\"{r}\\\"\" for i, r in enumerate(reviews)])\n",
    "    \n",
    "    prompt = f\"\"\"Classify the sentiment of each movie review below.\n",
    "For each review, respond with the number and sentiment (Positive/Negative/Mixed).\n",
    "\n",
    "Reviews:\n",
    "{formatted}\n",
    "\n",
    "Format your response as:\n",
    "1. [Sentiment]\n",
    "2. [Sentiment]\n",
    "...\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, provider=provider, temperature=0, max_tokens=500)\n",
    "    \n",
    "    # Parse results\n",
    "    results = []\n",
    "    for line in response.strip().split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and line[0].isdigit():\n",
    "            # Extract sentiment\n",
    "            for sentiment in [\"Positive\", \"Negative\", \"Mixed\", \"Neutral\"]:\n",
    "                if sentiment.lower() in line.lower():\n",
    "                    results.append(sentiment)\n",
    "                    break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test batch classification\n",
    "test_reviews = [\n",
    "    \"Amazing film! Best I've seen this year.\",\n",
    "    \"Terrible waste of time.\",\n",
    "    \"Visually stunning but boring story.\",\n",
    "    \"Perfect in every way.\",\n",
    "    \"Meh. It was okay.\"\n",
    "]\n",
    "\n",
    "sentiments = batch_classify(test_reviews)\n",
    "\n",
    "print(\"Batch Classification Results:\\n\")\n",
    "for review, sentiment in zip(test_reviews, sentiments):\n",
    "    print(f\"{sentiment:10} | {review[:50]}...\")\n",
    "\n",
    "print(f\"\\nClassified {len(test_reviews)} reviews in 1 API call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11.1: Calculate Cost Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the cost difference between:\n",
    "# 1. Individual API calls (one per review)\n",
    "# 2. Batched API calls (10 reviews per call)\n",
    "\n",
    "# Assume:\n",
    "# - Average prompt length: 50 tokens\n",
    "# - Average response length: 10 tokens per review\n",
    "# - Cost: $0.001 per 1K input tokens, $0.002 per 1K output tokens\n",
    "\n",
    "def calculate_cost_comparison(n_reviews, tokens_per_prompt=50, tokens_per_response=10):\n",
    "    \"\"\"\n",
    "    Calculate cost comparison between individual and batched calls.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Calculate for 1000 reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: Building a Complete Pipeline\n",
    "\n",
    "### Challenge: Movie Review Analysis Pipeline\n",
    "\n",
    "Build a complete pipeline that processes movie reviews through multiple stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Build a complete analysis pipeline\n",
    "\n",
    "class MovieReviewPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for movie review analysis.\n",
    "    \n",
    "    Stages:\n",
    "    1. Classification (sentiment)\n",
    "    2. Entity extraction (actors, directors)\n",
    "    3. Attribute labeling (acting, visuals, story)\n",
    "    4. Augmentation (paraphrases for training)\n",
    "    5. Summary generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, provider=\"gemini\"):\n",
    "        self.provider = provider\n",
    "        self.stats = {\n",
    "            \"api_calls\": 0,\n",
    "            \"reviews_processed\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "    \n",
    "    def classify_sentiment(self, review):\n",
    "        \"\"\"Stage 1: Sentiment classification.\"\"\"\n",
    "        self.stats[\"api_calls\"] += 1\n",
    "        return classify_sentiment_few_shot(review, self.provider)\n",
    "    \n",
    "    def extract_entities(self, review):\n",
    "        \"\"\"Stage 2: Named entity extraction.\"\"\"\n",
    "        self.stats[\"api_calls\"] += 1\n",
    "        return extract_entities(review, self.provider)\n",
    "    \n",
    "    def label_attributes(self, review):\n",
    "        \"\"\"Stage 3: Multi-attribute labeling.\"\"\"\n",
    "        self.stats[\"api_calls\"] += 1\n",
    "        return label_review_attributes(review, self.provider)\n",
    "    \n",
    "    def augment(self, review, n=2):\n",
    "        \"\"\"Stage 4: Generate paraphrases.\"\"\"\n",
    "        self.stats[\"api_calls\"] += 1\n",
    "        return generate_paraphrases(review, n, self.provider)\n",
    "    \n",
    "    def summarize(self, review):\n",
    "        \"\"\"Stage 5: Generate one-line summary.\"\"\"\n",
    "        self.stats[\"api_calls\"] += 1\n",
    "        prompt = f'Summarize this movie review in one sentence: \"{review}\"'\n",
    "        return generate_text(prompt, self.provider, temperature=0.3)\n",
    "    \n",
    "    def process_review(self, review, stages=[\"classify\", \"entities\", \"attributes\", \"summarize\"]):\n",
    "        \"\"\"\n",
    "        Process a single review through selected stages.\n",
    "        \"\"\"\n",
    "        result = {\"original\": review}\n",
    "        \n",
    "        try:\n",
    "            if \"classify\" in stages:\n",
    "                result[\"sentiment\"] = self.classify_sentiment(review)\n",
    "            \n",
    "            if \"entities\" in stages:\n",
    "                result[\"entities\"] = self.extract_entities(review)\n",
    "            \n",
    "            if \"attributes\" in stages:\n",
    "                result[\"attributes\"] = self.label_attributes(review)\n",
    "            \n",
    "            if \"augment\" in stages:\n",
    "                result[\"paraphrases\"] = self.augment(review)\n",
    "            \n",
    "            if \"summarize\" in stages:\n",
    "                result[\"summary\"] = self.summarize(review)\n",
    "            \n",
    "            self.stats[\"reviews_processed\"] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            result[\"error\"] = str(e)\n",
    "            self.stats[\"errors\"] += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get processing statistics.\"\"\"\n",
    "        return self.stats\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline = MovieReviewPipeline(provider=\"gemini\")\n",
    "\n",
    "# Process a review through all stages\n",
    "test_review = \"Heath Ledger's Joker is unforgettable. Christopher Nolan's best work.\"\n",
    "result = pipeline.process_review(test_review)\n",
    "\n",
    "print(f\"Pipeline Result for: {test_review}\\n\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"\\nStats: {pipeline.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Process Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Process all 15 reviews through the pipeline\n",
    "# Add rate limiting (1 second delay between reviews)\n",
    "# Track total processing time and costs\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems\n",
    "\n",
    "### Challenge 1: Hybrid Labeling System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Build a hybrid labeling system that:\n",
    "# 1. Uses LLM for initial labeling with confidence scores\n",
    "# 2. Sends low-confidence items to human review queue\n",
    "# 3. Uses majority voting when multiple LLM models disagree\n",
    "\n",
    "class HybridLabeler:\n",
    "    def __init__(self, confidence_threshold=0.8):\n",
    "        self.threshold = confidence_threshold\n",
    "        self.human_queue = []\n",
    "        self.auto_labeled = []\n",
    "    \n",
    "    def label(self, review):\n",
    "        \"\"\"\n",
    "        Label a review using hybrid approach.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def get_human_queue(self):\n",
    "        return self.human_queue\n",
    "    \n",
    "    def get_auto_labeled(self):\n",
    "        return self.auto_labeled\n",
    "\n",
    "# Test your hybrid labeler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: LLM-Based Data Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Use LLM to validate and fix movie data (Week 2 connection)\n",
    "\n",
    "messy_data = [\n",
    "    {\"title\": \"the godfather\", \"year\": \"1972\", \"rating\": \"9.2/10\"},\n",
    "    {\"title\": \"INCEPTION\", \"year\": \"two thousand ten\", \"rating\": \"good\"},\n",
    "    {\"title\": \"The Dark Knight  \", \"year\": \"2008\", \"rating\": \"9.0\"},\n",
    "    {\"title\": \"interstellar\", \"year\": \"14\", \"rating\": \"8.6 out of 10\"},\n",
    "]\n",
    "\n",
    "def validate_and_fix_with_llm(data, provider=\"gemini\"):\n",
    "    \"\"\"\n",
    "    Use LLM to validate and fix messy movie data.\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_data: List of properly formatted records\n",
    "        fixes_made: List of fixes that were applied\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Build a Review Generator for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Challenge: Generate synthetic movie reviews for testing\n# This is useful for testing your ML pipeline when real data is limited\n\ndef generate_synthetic_reviews(movie_title, sentiment, n=5, provider=\"gemini\"):\n    \"\"\"\n    Generate synthetic movie reviews with specified sentiment.\n    \n    Args:\n        movie_title: Name of the movie\n        sentiment: \"Positive\", \"Negative\", or \"Mixed\"\n        n: Number of reviews to generate\n    \n    Returns:\n        List of synthetic review texts\n    \"\"\"\n    # Your code here\n    pass\n\n# Generate 5 positive and 5 negative reviews for a new movie\n"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 13: Beyond Movies - Real-World LLM Applications\n\nNow let's explore the **true power of LLMs** across diverse tasks!\n\n### 13.1 Text Summarization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Text Summarization - Works on any content!\n\ndef summarize_text(text, max_sentences=3, provider=\"gemini\"):\n    \"\"\"Summarize any text into key points.\"\"\"\n    prompt = f\"\"\"Summarize this text in {max_sentences} sentences or less.\nFocus on the key points.\n\nText: {text}\n\nSummary:\"\"\"\n    return generate_text(prompt, provider=provider, temperature=0.3)\n\n# Example: Summarize a research abstract\nresearch_abstract = \"\"\"\nMachine learning models are increasingly being deployed in production environments,\nbut maintaining their performance over time remains challenging. This paper introduces\na novel approach to continuous model monitoring using statistical drift detection\ncombined with automated retraining pipelines. Our method achieves 95% accuracy in\ndetecting performance degradation within 24 hours, compared to 48+ hours for baseline\napproaches. We validate our approach on three real-world datasets spanning recommendation\nsystems, fraud detection, and natural language processing. Results show a 40% reduction\nin model downtime and 25% improvement in overall system reliability.\n\"\"\"\n\nsummary = summarize_text(research_abstract)\nprint(\"Original (Research Abstract):\")\nprint(research_abstract[:200] + \"...\")\nprint(f\"\\nSummary:\\n{summary}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 13.2 Translation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Translation - Any language pair!\n\ndef translate(text, source_lang, target_lang, provider=\"gemini\"):\n    \"\"\"Translate text between languages.\"\"\"\n    prompt = f\"\"\"Translate this text from {source_lang} to {target_lang}.\nOnly provide the translation, no explanations.\n\nText: {text}\n\nTranslation:\"\"\"\n    return generate_text(prompt, provider=provider, temperature=0.3)\n\n# Examples\nexamples = [\n    (\"Hello, how are you today?\", \"English\", \"Hindi\"),\n    (\"Machine learning is transforming industries.\", \"English\", \"Spanish\"),\n    (\"Bonjour, je m'appelle Claude.\", \"French\", \"English\"),\n]\n\nprint(\"Translation Examples:\\n\")\nfor text, source, target in examples:\n    translation = translate(text, source, target)\n    print(f\"{source}: {text}\")\n    print(f\"{target}: {translation}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 13.3 Code Generation & Explanation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Code Generation - Write code from descriptions!\n\ndef generate_code(description, language=\"Python\", provider=\"gemini\"):\n    \"\"\"Generate code from natural language description.\"\"\"\n    prompt = f\"\"\"Write {language} code that does the following:\n\n{description}\n\nOnly provide the code with comments. No explanations outside the code.\"\"\"\n    return generate_text(prompt, provider=provider, temperature=0.3)\n\ndef explain_code(code, provider=\"gemini\"):\n    \"\"\"Explain what a piece of code does.\"\"\"\n    prompt = f\"\"\"Explain what this code does in simple terms:\n\n```\n{code}\n```\n\nExplanation:\"\"\"\n    return generate_text(prompt, provider=provider, temperature=0.3)\n\n# Example 1: Generate code\ndescription = \"Calculate the Fibonacci sequence up to n terms and return as a list\"\ncode = generate_code(description)\nprint(f\"Task: {description}\")\nprint(f\"\\nGenerated Code:\\n{code}\")\n\n# Example 2: Explain code\nmystery_code = \"\"\"\ndef f(x):\n    return x if x <= 1 else f(x-1) + f(x-2)\n\"\"\"\nexplanation = explain_code(mystery_code)\nprint(f\"\\n\\nCode to explain: {mystery_code}\")\nprint(f\"Explanation: {explanation}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 13.4 Question Answering from Context",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Question Answering - Extract answers from documents!\n\ndef answer_question(context, question, provider=\"gemini\"):\n    \"\"\"Answer a question based on given context.\"\"\"\n    prompt = f\"\"\"Based on the following context, answer the question.\nIf the answer is not in the context, say \"Not found in context.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n    return generate_text(prompt, provider=provider, temperature=0)\n\n# Example: Company FAQ\ncompany_context = \"\"\"\nTechStart Inc. was founded in 2018 by Dr. Sarah Chen and Mark Williams.\nThe company is headquartered in Bangalore, India with offices in San Francisco and London.\nTechStart specializes in AI-powered customer service solutions and has raised $50 million\nin Series B funding. The company has 250 employees and serves over 500 enterprise clients.\nTheir flagship product, ChatAssist Pro, handles 10 million customer interactions monthly.\nOffice hours are 9 AM to 6 PM IST, Monday through Friday.\n\"\"\"\n\nquestions = [\n    \"Who founded TechStart?\",\n    \"How much funding has the company raised?\",\n    \"Where is the headquarters?\",\n    \"What is their main product?\",\n    \"What is the company's stock price?\"  # Not in context\n]\n\nprint(\"Q&A from Company Document:\\n\")\nfor q in questions:\n    answer = answer_question(company_context, q)\n    print(f\"Q: {q}\")\n    print(f\"A: {answer}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 13.5 Structured Data Extraction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Structured Data Extraction - Parse unstructured text to JSON!\n\ndef extract_structured_data(text, schema_description, provider=\"gemini\"):\n    \"\"\"Extract structured data from unstructured text.\"\"\"\n    prompt = f\"\"\"Extract information from this text into structured JSON.\n\nText: {text}\n\nExtract these fields: {schema_description}\n\nRespond with only valid JSON.\"\"\"\n    response = generate_text(prompt, provider=provider, temperature=0)\n    try:\n        cleaned = response.strip()\n        if cleaned.startswith(\"```\"):\n            cleaned = cleaned.split(\"```\")[1]\n            if cleaned.startswith(\"json\"):\n                cleaned = cleaned[4:]\n        return json.loads(cleaned)\n    except:\n        return {\"raw\": response}\n\n# Example 1: Extract from email\nemail = \"\"\"\nHi Team,\n\nJust wanted to confirm our meeting for next Tuesday, December 10th at 2:30 PM.\nWe'll be discussing the Q4 budget review in Conference Room B.\n\nPlease bring your laptops and the latest sales reports.\n\nThanks,\nJennifer Martinez\nSenior Project Manager\n\"\"\"\n\nschema = \"sender_name, sender_role, meeting_date, meeting_time, meeting_topic, location, required_items\"\nextracted = extract_structured_data(email, schema)\nprint(\"Email Extraction:\")\nprint(json.dumps(extracted, indent=2))\n\n# Example 2: Extract from job posting\njob_posting = \"\"\"\nWe're hiring a Senior Machine Learning Engineer at Google Bangalore.\nRequirements: 5+ years experience, Python, PyTorch, distributed systems.\nSalary: 40-60 LPA. Remote-friendly. Apply by January 15, 2025.\n\"\"\"\n\nschema = \"job_title, company, location, experience_required, skills, salary_range, deadline, remote_policy\"\nextracted = extract_structured_data(job_posting, schema)\nprint(\"\\nJob Posting Extraction:\")\nprint(json.dumps(extracted, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 13.6 Intent Classification & Routing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Intent Classification - Route messages to the right department!\n\ndef classify_intent(message, intents, provider=\"gemini\"):\n    \"\"\"Classify user message intent for routing.\"\"\"\n    intent_list = \", \".join(intents)\n    prompt = f\"\"\"Classify this customer message into one of these intents: {intent_list}\n\nMessage: \"{message}\"\n\nRespond with JSON:\n{{\"intent\": \"chosen_intent\", \"confidence\": 0.0-1.0, \"reasoning\": \"brief explanation\"}}\n\nOnly JSON.\"\"\"\n    response = generate_text(prompt, provider=provider, temperature=0)\n    try:\n        cleaned = response.strip()\n        if cleaned.startswith(\"```\"):\n            cleaned = cleaned.split(\"```\")[1].strip()\n            if cleaned.startswith(\"json\"):\n                cleaned = cleaned[4:].strip()\n        return json.loads(cleaned)\n    except:\n        return {\"raw\": response}\n\n# Customer support routing example\nintents = [\"billing\", \"technical_support\", \"sales\", \"returns\", \"general_inquiry\"]\n\nmessages = [\n    \"My credit card was charged twice for the same order!\",\n    \"The app keeps crashing when I try to upload photos.\",\n    \"I'd like to know about enterprise pricing for 100+ users.\",\n    \"I want to return the headphones I bought last week.\",\n    \"What are your office hours?\",\n    \"The product I received is different from what I ordered.\"\n]\n\nprint(\"Customer Support Intent Classification:\\n\")\nfor msg in messages:\n    result = classify_intent(msg, intents)\n    if isinstance(result, dict) and \"intent\" in result:\n        print(f\"Message: {msg[:50]}...\")\n        print(f\"  -> Intent: {result['intent']} (confidence: {result.get('confidence', 'N/A')})\\\\n\")\n    else:\n        print(f\"Message: {msg[:50]}... -> {result}\\\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Question 13.1: Build Your Own Application\n\nChoose one of these mini-projects and implement it using the techniques above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TODO: Choose and implement ONE of these mini-projects:\n\n# Option A: Resume Parser\n# - Extract name, email, skills, education, experience from resume text\n# - Return structured JSON\n\n# Option B: Recipe Assistant\n# - Take a list of ingredients\n# - Generate a recipe that uses those ingredients\n# - Include cooking time and difficulty level\n\n# Option C: Study Assistant\n# - Take a paragraph of educational content\n# - Generate 5 quiz questions with answers\n# - Vary difficulty levels\n\n# Option D: Sentiment-Aware Chatbot\n# - Detect user sentiment from their message\n# - Respond appropriately (empathetic if negative, enthusiastic if positive)\n\n# Your implementation here:\n\ndef my_application(input_data, provider=\"gemini\"):\n    \"\"\"\n    Your mini-project implementation.\n    \"\"\"\n    # Your code here\n    pass\n\n# Test your application:\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **API Setup**: Gemini (free tier) and OpenRouter (free models)\n",
    "2. **Prompt Engineering**: Zero-shot, few-shot, chain-of-thought\n",
    "3. **Data Labeling**: LLMs as 10-100x faster labelers (Week 3-4 connection)\n",
    "4. **Data Augmentation**: Paraphrases and style transfer (Week 5 connection)\n",
    "5. **Structured Output**: JSON extraction with Pydantic\n",
    "6. **Cost Optimization**: Batching, model selection\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Task | LLM Approach | Cost Savings |\n",
    "|------|--------------|-------------|\n",
    "| Sentiment Classification | Zero-shot/Few-shot | Free with Gemini! |\n",
    "| Data Labeling | LLM + confidence filtering | 10-100x faster than humans |\n",
    "| Text Augmentation | Paraphrase generation | More natural than rule-based |\n",
    "| Data Validation | LLM-based fixing | Handles edge cases |\n",
    "\n",
    "### Free Resources\n",
    "\n",
    "- **Gemini API**: [aistudio.google.com/apikey](https://aistudio.google.com/apikey) - 15 RPM free\n",
    "- **OpenRouter**: [openrouter.ai](https://openrouter.ai) - Many free models\n",
    "- **Hugging Face**: Local models with Transformers library\n",
    "\n",
    "### Next Week\n",
    "\n",
    "Week 7: Model Development - Train your own models with the labeled data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}