{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 Lab: Profiling & Optimization\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will learn to:\n",
    "1. **Profile training loops** to find bottlenecks\n",
    "2. **Optimize data loading** with DataLoader settings\n",
    "3. **Use mixed precision** training (AMP)\n",
    "4. **Apply torch.compile** for faster execution\n",
    "\n",
    "**Goal**: Optimize a ResNet training loop from baseline to 2-3x faster.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Baseline Measurement\n",
    "\n",
    "Establish a baseline before optimizing.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                 Training Pipeline                       │\n",
    "│                                                         │\n",
    "│  Data Loading ──► Forward Pass ──► Backward ──► Update  │\n",
    "│     (CPU)          (GPU)          (GPU)        (GPU)    │\n",
    "│                                                         │\n",
    "│  Bottleneck?      Bottleneck?   Bottleneck?  Bottleneck?│\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 (Solved): Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 0  # Start with 0 for baseline\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Baseline DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False  # Baseline without optimization\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # CIFAR-10 has 10 classes\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (Solved): Baseline Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return metrics.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        batch_start = time.perf_counter()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        batch_time = time.perf_counter() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)}, \"\n",
    "                  f\"Loss: {loss.item():.4f}, \"\n",
    "                  f\"Batch time: {batch_time*1000:.2f}ms\")\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': 100. * correct / total,\n",
    "        'throughput': total / sum(batch_times),\n",
    "        'batch_times': batch_times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline training\n",
    "print(\"=\"*50)\n",
    "print(\"BASELINE TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "baseline_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "baseline_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"  Epoch time: {baseline_time:.2f}s\")\n",
    "print(f\"  Throughput: {baseline_metrics['throughput']:.2f} samples/sec\")\n",
    "print(f\"  Accuracy: {baseline_metrics['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Data Loading Optimization\n",
    "\n",
    "Data loading is often the bottleneck.\n",
    "\n",
    "## 2.1 num_workers Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Test Different num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Test num_workers = [0, 2, 4, 8] and record throughput\n",
    "\n",
    "worker_configs = [0, 2, 4, 8]\n",
    "worker_results = []\n",
    "\n",
    "for num_workers in worker_configs:\n",
    "    print(f\"\\nTesting num_workers={num_workers}\")\n",
    "    \n",
    "    # Create dataloader with new config\n",
    "    \n",
    "    # Measure throughput (just iterate through data, no training)\n",
    "    \n",
    "    # Record results\n",
    "    pass\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Add pin_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compare with and without pin_memory=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Mixed Precision Training (AMP)\n",
    "\n",
    "Use FP16 for faster training with minimal accuracy loss.\n",
    "\n",
    "## 3.1 Enable AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): AMP Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "def train_epoch_amp(model, dataloader, criterion, optimizer, device, use_amp=True):\n",
    "    \"\"\"Train with optional AMP.\"\"\"\n",
    "    model.train()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        batch_start = time.perf_counter()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward with autocast\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward with gradient scaling\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        batch_times.append(time.perf_counter() - batch_start)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': 100. * correct / total,\n",
    "        'throughput': total / sum(batch_times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Compare FP32 vs AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compare training with and without AMP\n",
    "# Measure: throughput, memory usage, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: torch.compile (PyTorch 2.0+)\n",
    "\n",
    "Compile the model for optimized execution.\n",
    "\n",
    "## 4.1 Using torch.compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Apply torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    # Create fresh model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Compile model\n",
    "    compiled_model = torch.compile(model, mode='default')\n",
    "    print(\"Model compiled successfully!\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Benchmark compiled vs non-compiled model\n",
    "    \n",
    "else:\n",
    "    print(\"torch.compile requires PyTorch 2.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Profiling with PyTorch Profiler\n",
    "\n",
    "Use the profiler to identify bottlenecks.\n",
    "\n",
    "## 5.1 Profile Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (Solved): Profile Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if torch.cuda.is_available():\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if batch_idx >= 10:  # Profile first 10 batches\n",
    "            break\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        with record_function(\"forward\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        with record_function(\"backward\"):\n",
    "            loss.backward()\n",
    "        \n",
    "        with record_function(\"optimizer_step\"):\n",
    "            optimizer.step()\n",
    "\n",
    "# Print top operations\n",
    "sort_key = \"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\"\n",
    "print(prof.key_averages().table(sort_by=sort_key, row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Analyze Profile Results\n",
    "\n",
    "Based on the profiler output:\n",
    "1. What is the most time-consuming operation?\n",
    "2. Is data transfer (HtoD) significant?\n",
    "3. Where would you focus optimization efforts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANALYSIS HERE\n",
    "analysis = {\n",
    "    \"most_time_consuming_op\": None,  # Fill in\n",
    "    \"data_transfer_significant\": None,  # True or False\n",
    "    \"optimization_focus\": None  # Describe\n",
    "}\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Comprehensive Comparison\n",
    "\n",
    "Compare all optimizations.\n",
    "\n",
    "## 6.1 Final Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1: Create Comparison Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Run all configurations and create a comparison\n",
    "\n",
    "configs = [\n",
    "    {'name': 'Baseline', 'num_workers': 0, 'pin_memory': False, 'use_amp': False},\n",
    "    {'name': 'Optimized DataLoader', 'num_workers': 4, 'pin_memory': True, 'use_amp': False},\n",
    "    {'name': '+ AMP', 'num_workers': 4, 'pin_memory': True, 'use_amp': True},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting: {config['name']}\")\n",
    "    # Run training and collect metrics\n",
    "    pass\n",
    "\n",
    "# Create comparison chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Baseline measurement**: Establish metrics before optimizing\n",
    "2. **DataLoader optimization**: num_workers, pin_memory\n",
    "3. **Mixed precision (AMP)**: 2x speedup with minimal accuracy loss\n",
    "4. **torch.compile**: PyTorch 2.0 compilation\n",
    "5. **Profiling**: Finding bottlenecks with PyTorch Profiler\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Optimization | Expected Speedup | Complexity |\n",
    "|--------------|------------------|------------|\n",
    "| num_workers | 1.5-2x | Low |\n",
    "| pin_memory | 1.1-1.3x | Low |\n",
    "| AMP | 1.5-2.5x | Medium |\n",
    "| torch.compile | 1.2-1.5x | Low |\n",
    "\n",
    "**Combined**: 2-3x total speedup!\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit:\n",
    "1. This completed notebook\n",
    "2. Comparison chart showing speedups\n",
    "3. Brief report: Which optimizations gave the best results on your hardware?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
